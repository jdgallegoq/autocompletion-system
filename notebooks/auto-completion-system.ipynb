{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation: auto-completion system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OS\n",
    "import os\n",
    "\n",
    "# Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pytorch version: \", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.s3_class import S3Functions\n",
    "\n",
    "s3_funcs = S3Functions(bucket_name='jdgallegoq-text-gen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x17e0d76d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reproduce same results\n",
    "SEED = 42\n",
    "# seed on torch\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64776"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "with s3_funcs.read_object('Dailog-dataset.dialogs_dataset') as f:\n",
    "    dialogs = pickle.load(f)\n",
    "len(dialogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how much is the pizza going to cost',\n",
       " 'the Westgate one please',\n",
       " 'No Juliet, the heated swimming pool is key in our dinner can we find another option around there?',\n",
       " 'Great! Thank you for doing this for me',\n",
       " 'I really want Mediterranean food tonight, somewhere that takes reservations',\n",
       " 'I looking for a theater in Naperville IL',\n",
       " ' Thanks so much! ',\n",
       " \"Alright, I'll see you soon\",\n",
       " 'He did not say who would leave that inheritance in his will?',\n",
       " \"Awesome! You're the best\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print a sample of 10\n",
    "random.sample(dialogs, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text cleaning\n",
    "dialogs_clean = []\n",
    "\n",
    "for i in dialogs:\n",
    "    # remove everything except alphabets\n",
    "    i = re.sub(\"[^a-zA-Z' ]\", \"\", i)\n",
    "    # convert text to lowercase\n",
    "    i = i.lower()\n",
    "    # add cleaned text to the list\n",
    "    dialogs_clean.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' i appreciate your help',\n",
       " 'can you get me tickets for a movie',\n",
       " 'beef and veggie',\n",
       " \"it's ok for me\",\n",
       " 'pepperoni and onions',\n",
       " ' get me the next appointment at intelligent auto solutions',\n",
       " 'hey can you find me a restaurant',\n",
       " 'book an appointment with intelligent auto solutions',\n",
       " 'that sounds about right',\n",
       " 'hi i have just landed at slc airport']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print a sample of 10\n",
    "random.sample(dialogs_clean, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# term frequency\n",
    "all_words = \" \".join(dialogs_clean).split()\n",
    "\n",
    "# create dictionary\n",
    "words_dict = {}\n",
    "for word in all_words:\n",
    "    if word in words_dict:\n",
    "        # increment count by 1\n",
    "        words_dict[word] = words_dict[word] + 1\n",
    "    else:\n",
    "        words_dict[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uppermiddle</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shoots</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>geesh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word  count\n",
       "0  uppermiddle      1\n",
       "1       shoots      1\n",
       "2        geesh      1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprare a dataset\n",
    "words_df = pd.DataFrame({\n",
    "    \"word\": list(words_dict.keys()),\n",
    "    \"count\": list(words_dict.values())\n",
    "})\n",
    "# sort\n",
    "words_df = words_df.sort_values(by='count')\n",
    "words_df.reset_index(inplace=True, drop=True)\n",
    "words_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11145</th>\n",
       "      <td>the</td>\n",
       "      <td>15406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11146</th>\n",
       "      <td>i</td>\n",
       "      <td>19654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word  count\n",
       "11145  the  15406\n",
       "11146    i  19654"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11147"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab size\n",
    "len(words_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rare words distribution in the vocabulary: 69.03\n",
      "Rare words coverage in the corpus: 2.27\n"
     ]
    }
   ],
   "source": [
    "# find and replace rare tokens\n",
    "# replace with UNKNOW TOKEN\n",
    "# define rarity threshold\n",
    "rare_thres = 4\n",
    "\n",
    "# get percentage of rare tokens\n",
    "rare_words_count = len(words_df[words_df['count']<rare_thres]['word'])\n",
    "total_word = len(words_df)\n",
    "rare_dist = rare_words_count / total_word\n",
    "\n",
    "rare_cover = words_df[words_df['count'] < rare_thres]['count'].sum()/words_df['count'].sum()\n",
    "\n",
    "print(f\"Rare words distribution in the vocabulary: {rare_dist*100:.2f}\")\n",
    "print(f\"Rare words coverage in the corpus: {rare_cover*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract rare words in a list\n",
    "rare_words = words_df[words_df['count'] < rare_thres]['word'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64776/64776 [00:10<00:00, 6324.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# create a pattern with regex\n",
    "pattern = \"\"\n",
    "for i in rare_words:\n",
    "    pattern += \" {} |\".format(i)\n",
    "\n",
    "# remove the last '|' from the pattern\n",
    "pattern = pattern[:-1]\n",
    "\n",
    "# empty list for clean text\n",
    "dialogs_clean_no_rare_words = []\n",
    "for d in tqdm(dialogs_clean):\n",
    "    text = re.sub(pattern, \" <unk> \", d)\n",
    "    dialogs_clean_no_rare_words.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['does it serve traditional chinese dessert',\n",
       " 'how much extra time to reach <unk> ',\n",
       " 'ok lets reserve a table for dinner at hakkasan',\n",
       " 'hello i need to get a car please',\n",
       " 'holiday inn <unk> parkconv <unk> convention center drive <unk> park il',\n",
       " 'bowling alley <unk> highway <unk> park il',\n",
       " 'what types of cars does uber have',\n",
       " \"what's the price difference\",\n",
       " 'ok get me the cheapest please',\n",
       " 'ok then get me the next level']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogs_clean_no_rare_words[520:530]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprare sequences\n",
    "# 1. define sequence lenght based on distribution of sentence lenghts\n",
    "def create_seq(text, seq_lenght=5):\n",
    "    sequence = []\n",
    "    if len(text.split())>seq_lenght:\n",
    "        for i in range(seq_lenght, len(text.split())):\n",
    "            seq = text.split()[i-seq_lenght: i+1]\n",
    "            sequence.append(\" \".join(seq))\n",
    "\n",
    "        return sequence\n",
    "\n",
    "    else:\n",
    "        return [text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = [create_seq(i) for i in dialogs_clean_no_rare_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"hi i'm looking to book a\",\n",
       "  \"i'm looking to book a table\",\n",
       "  'looking to book a table for',\n",
       "  'to book a table for korean',\n",
       "  'book a table for korean fod'],\n",
       " ['somewhere in southern nyc maybe the',\n",
       "  'in southern nyc maybe the east',\n",
       "  'southern nyc maybe the east village'],\n",
       " [\"we don't want to sit at\",\n",
       "  \"don't want to sit at the\",\n",
       "  'want to sit at the bar',\n",
       "  'to sit at the bar but',\n",
       "  'sit at the bar but anywhere',\n",
       "  'at the bar but anywhere else',\n",
       "  'the bar but anywhere else is',\n",
       "  'bar but anywhere else is fine'],\n",
       " ['what times are available'],\n",
       " [\"yikes we can't do those times\"],\n",
       " ['let me check'],\n",
       " [\"great let's book that\"],\n",
       " [\"no that's it just book\"],\n",
       " ['hi i would like to see',\n",
       "  'i would like to see if',\n",
       "  'would like to see if the',\n",
       "  'like to see if the movie',\n",
       "  'to see if the movie what',\n",
       "  'see if the movie what men',\n",
       "  'if the movie what men want',\n",
       "  'the movie what men want is',\n",
       "  'movie what men want is playing',\n",
       "  'what men want is playing here'],\n",
       " ['yes for me and a friend',\n",
       "  'for me and a friend so',\n",
       "  'me and a friend so two',\n",
       "  'and a friend so two tickets',\n",
       "  'a friend so two tickets please']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"hi i'm looking to book a\",\n",
       " \"i'm looking to book a table\",\n",
       " 'looking to book a table for',\n",
       " 'to book a table for korean',\n",
       " 'book a table for korean fod',\n",
       " 'somewhere in southern nyc maybe the',\n",
       " 'in southern nyc maybe the east',\n",
       " 'southern nyc maybe the east village',\n",
       " \"we don't want to sit at\",\n",
       " \"don't want to sit at the\",\n",
       " 'want to sit at the bar',\n",
       " 'to sit at the bar but',\n",
       " 'sit at the bar but anywhere',\n",
       " 'at the bar but anywhere else',\n",
       " 'the bar but anywhere else is']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge to a single list\n",
    "seqs = sum(seqs, [])\n",
    "seqs[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205346"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count sequences\n",
    "len(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"hi i'm looking to book\", \"i'm looking to book a\")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create input and target sequences (x and y)\n",
    "x = []\n",
    "y = []\n",
    "for s in seqs:\n",
    "    x.append(\" \".join(s.split()[:-1]))\n",
    "    y.append(\" \".join(s.split()[1:]))\n",
    "\n",
    "x[0], y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 'bald')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create token-integer mappings\n",
    "# text-gen works like generation integers\n",
    "# that need to be converted into tokens by mapping\n",
    "# the vocabulary\n",
    "\n",
    "int2token = {}\n",
    "count = 1\n",
    "\n",
    "for w in set(\" \".join(dialogs_clean_no_rare_words).split()):\n",
    "    int2token[count] = w\n",
    "    count += 1\n",
    "\n",
    "# token to integer mapping (inverse of above)\n",
    "token2int = {v: k for k, v in int2token.items()}\n",
    "\n",
    "token2int['can'], int2token[1127]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_thres = 150000\n",
    "\n",
    "x_train = x[:split_thres]\n",
    "x_val = x[split_thres:]\n",
    "y_train = y[:split_thres]\n",
    "y_val = y[split_thres:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad sequences\n",
    "Explore sequence lenghts to get the optimal padding lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(i.split()) for i in x_train])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max lenght should be 5\n",
    "max_text_len = 5\n",
    "\n",
    "def pad_sequences(seq, n):\n",
    "    # get tokens from seq\n",
    "    seq = seq.split()\n",
    "    # check if seq len < n\n",
    "    if len(seq)<n:\n",
    "        for i in range(n-len(seq)):\n",
    "            seq.append(\"<pad>\")\n",
    "    return \" \".join(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad sequences\n",
    "x_train_pad = [pad_sequences(s, max_text_len) for s in x_train]\n",
    "y_train_pad = [pad_sequences(s, max_text_len) for s in y_train]\n",
    "\n",
    "x_val_pad = [pad_sequences(s, max_text_len) for s in x_val]\n",
    "y_val_pad = [pad_sequences(s, max_text_len) for s in y_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"hi i'm looking to book\",\n",
       " \"i'm looking to book a\",\n",
       " 'looking to book a table',\n",
       " 'to book a table for',\n",
       " 'book a table for korean',\n",
       " 'somewhere in southern nyc maybe',\n",
       " 'in southern nyc maybe the',\n",
       " 'southern nyc maybe the east',\n",
       " \"we don't want to sit\",\n",
       " \"don't want to sit at\",\n",
       " 'want to sit at the',\n",
       " 'to sit at the bar',\n",
       " 'sit at the bar but',\n",
       " 'at the bar but anywhere',\n",
       " 'the bar but anywhere else',\n",
       " 'bar but anywhere else is',\n",
       " 'what times are <pad> <pad>',\n",
       " \"yikes we can't do those\",\n",
       " 'let me <pad> <pad> <pad>',\n",
       " \"great let's book <pad> <pad>\"]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_pad[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"i'm looking to book a\",\n",
       " 'looking to book a table',\n",
       " 'to book a table for',\n",
       " 'book a table for korean',\n",
       " 'a table for korean fod',\n",
       " 'in southern nyc maybe the',\n",
       " 'southern nyc maybe the east',\n",
       " 'nyc maybe the east village',\n",
       " \"don't want to sit at\",\n",
       " 'want to sit at the',\n",
       " 'to sit at the bar',\n",
       " 'sit at the bar but',\n",
       " 'at the bar but anywhere',\n",
       " 'the bar but anywhere else',\n",
       " 'bar but anywhere else is',\n",
       " 'but anywhere else is fine',\n",
       " 'times are available <pad> <pad>',\n",
       " \"we can't do those times\",\n",
       " 'me check <pad> <pad> <pad>',\n",
       " \"let's book that <pad> <pad>\"]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pad[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update vocab\n",
    "int2token[0] = \"<pad>\"\n",
    "token2int[\"<pad>\"] = 0\n",
    "# get vocab size\n",
    "vocab_size = len(int2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert padded sequences to integers\n",
    "def get_integer_seq(seq):\n",
    "    return [token2int[w] for w in seq.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply above function to x and y\n",
    "x_train_int = [get_integer_seq(s) for s in x_train_pad]\n",
    "y_train_int = [get_integer_seq(s) for s in y_train_pad]\n",
    "x_val_int = [get_integer_seq(s) for s in x_val_pad]\n",
    "y_val_int = [get_integer_seq(s) for s in y_val_pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5901, 2632, 2554, 2894, 5781],\n",
       " [2632, 2554, 2894, 5781, 1031],\n",
       " [2554, 2894, 5781, 1031, 549],\n",
       " [2894, 5781, 1031, 549, 5428],\n",
       " [5781, 1031, 549, 5428, 775],\n",
       " [2715, 1324, 2515, 4228, 5350],\n",
       " [1324, 2515, 4228, 5350, 3656],\n",
       " [2515, 4228, 5350, 3656, 815],\n",
       " [5435, 1239, 5790, 2894, 87],\n",
       " [1239, 5790, 2894, 87, 3969]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_int[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2632, 2554, 2894, 5781, 1031],\n",
       " [2554, 2894, 5781, 1031, 549],\n",
       " [2894, 5781, 1031, 549, 5428],\n",
       " [5781, 1031, 549, 5428, 775],\n",
       " [1031, 549, 5428, 775, 4040],\n",
       " [1324, 2515, 4228, 5350, 3656],\n",
       " [2515, 4228, 5350, 3656, 815],\n",
       " [4228, 5350, 3656, 815, 1525],\n",
       " [1239, 5790, 2894, 87, 3969],\n",
       " [5790, 2894, 87, 3969, 3656]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_int[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150000, 5), (150000, 5), (55346, 5), (55346, 5))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert lists into arrays\n",
    "x_train_int = np.array(x_train_int)\n",
    "y_train_int = np.array(y_train_int)\n",
    "x_val_int = np.array(x_val_int)\n",
    "y_val_int = np.array(y_val_int)\n",
    "\n",
    "x_train_int.shape, y_train_int.shape, x_val_int.shape, y_val_int.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "-> embedding layer:</br>\n",
    "    - input dim: vocab size</br>\n",
    "    - output dim: 200 (hyperparameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordLSTM(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_hidden=256,\n",
    "            n_layers=2,\n",
    "            drop_prob=0.3,\n",
    "            lr=1e-3\n",
    "        ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "\n",
    "        self.emb_layer = nn.Embedding(vocab_size, 200)\n",
    "\n",
    "        # define LSTM\n",
    "        # as input shape is (batch size, seq len, num features)\n",
    "        # set batch_first=True\n",
    "        self.lstm = nn.LSTM(200, n_hidden, n_layers, batch_first=True)\n",
    "\n",
    "        # Droput layer\n",
    "        self.droput = nn.Dropout(drop_prob)\n",
    "\n",
    "        # Dense layer\n",
    "        self.fc = nn.Linear(n_hidden, vocab_size)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        # get embed layer\n",
    "        embedded = self.emb_layer(x)\n",
    "        # pass it to LSTM\n",
    "        lstm_out, hidden = self.lstm(embedded, hidden)\n",
    "        # pass lstm_out to dropout\n",
    "        out = self.droput(lstm_out)\n",
    "        # reshape (batch size*seq_len, hidden_units)\n",
    "        out = out.reshape(-1, self.n_hidden)\n",
    "        # pass out to dense\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "            initialize first hidden and cell state for LSTM\n",
    "        \"\"\"\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        if (torch.cuda.is_available()):\n",
    "            hidden = (\n",
    "                # this first one for the hidden state\n",
    "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                # this second one for the cell state\n",
    "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda()\n",
    "            )\n",
    "        else:\n",
    "            hidden = (\n",
    "                # this first one for the hidden state\n",
    "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                # this second one for the cell state\n",
    "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_()\n",
    "            )\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordLSTM(\n",
      "  (emb_layer): Embedding(6502, 200)\n",
      "  (lstm): LSTM(200, 256, num_layers=2, batch_first=True)\n",
      "  (droput): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=6502, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# summary\n",
    "net = WordLSTM()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to pass batches\n",
    "def get_batches(x_array, y_array, batch_size):\n",
    "    prv = 0\n",
    "    for n in range(batch_size, x_array.shape[0], batch_size):\n",
    "        # batch for input\n",
    "        x = x_array[prv:n, :]\n",
    "        # batch for target\n",
    "        y = y_array[prv:n, :]\n",
    "        prv = n\n",
    "\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    net,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    lr=1e-3,\n",
    "    print_every=32,\n",
    "    save_path='../module/models'\n",
    "):\n",
    "    # set initial loss to infinite\n",
    "    best_valid_loss = float('inf')\n",
    "    # optimizer\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    # loss funtion\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # if GPU available push model to GPU\n",
    "    if (torch.cuda.is_available()):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    net.train()\n",
    "    for e in range(epochs):\n",
    "        # iterate over batches\n",
    "        for x, y in get_batches(x_train_int, y_train_int, batch_size):\n",
    "            counter += 1\n",
    "            # convert arrays to tensors\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            # if GPU available then push tensors to GPU\n",
    "            if (torch.cuda.is_available()):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            \n",
    "            # initialize hidden state\n",
    "            # this prevents to carry on information from\n",
    "            # previous batches\n",
    "            h = net.init_hidden(batch_size)\n",
    "            # set accumulated gradients to zero\n",
    "            net.zero_grad()\n",
    "            # get output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            # calculate loss and do backward\n",
    "            loss = criterion(output, targets.view(-1))\n",
    "            loss.backward()\n",
    "            # optimier step\n",
    "            opt.step()\n",
    "\n",
    "            if counter % print_every == 0:\n",
    "                # get val loss\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "\n",
    "                for x, y in get_batches(x_val_int, y_val_int, batch_size):\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    val_h = net.init_hidden(batch_size)\n",
    "\n",
    "                    inputs, targets = x, y\n",
    "                    if (torch.cuda.is_available()):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                    \n",
    "                    output, val_h = net(inputs, val_h)\n",
    "\n",
    "                    val_loss = criterion(output, targets.view(-1))\n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                # save best model\n",
    "                if np.mean(val_losses) < best_valid_loss:\n",
    "                    best_valid_loss = np.mean(val_losses)\n",
    "                    torch.save(net.state_dict(), f'{save_path}/saved_weights.pt')\n",
    "                \n",
    "                net.train()\n",
    "\n",
    "                print(\n",
    "                    \"Epoch: {}/{}\".format(e+1, epochs),\n",
    "                    \"Step: {}...\".format(counter),\n",
    "                    \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                    \"ppl: {:.4f} \".format(np.exp(np.mean(val_losses))),\n",
    "                    \"Val Loss: {:.4f}\".format(np.mean(val_losses))\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 Step: 32... Loss: 6.4171... ppl: 686.5443  Val Loss: 6.5317\n",
      "Epoch: 1/10 Step: 64... Loss: 5.6239... ppl: 342.2799  Val Loss: 5.8356\n",
      "Epoch: 1/10 Step: 96... Loss: 6.0970... ppl: 273.6285  Val Loss: 5.6118\n",
      "Epoch: 1/10 Step: 128... Loss: 5.2977... ppl: 238.7068  Val Loss: 5.4752\n",
      "Epoch: 1/10 Step: 160... Loss: 6.1782... ppl: 210.6979  Val Loss: 5.3504\n",
      "Epoch: 1/10 Step: 192... Loss: 5.7520... ppl: 186.9139  Val Loss: 5.2306\n",
      "Epoch: 1/10 Step: 224... Loss: 5.1411... ppl: 167.1139  Val Loss: 5.1187\n",
      "Epoch: 1/10 Step: 256... Loss: 4.2456... ppl: 151.4281  Val Loss: 5.0201\n",
      "Epoch: 1/10 Step: 288... Loss: 4.9037... ppl: 138.7299  Val Loss: 4.9325\n",
      "Epoch: 1/10 Step: 320... Loss: 4.9328... ppl: 128.7778  Val Loss: 4.8581\n",
      "Epoch: 1/10 Step: 352... Loss: 4.3324... ppl: 119.6864  Val Loss: 4.7849\n",
      "Epoch: 1/10 Step: 384... Loss: 4.9985... ppl: 112.2930  Val Loss: 4.7211\n",
      "Epoch: 1/10 Step: 416... Loss: 4.9008... ppl: 107.2934  Val Loss: 4.6756\n",
      "Epoch: 1/10 Step: 448... Loss: 4.5394... ppl: 102.8382  Val Loss: 4.6332\n",
      "Epoch: 1/10 Step: 480... Loss: 4.2631... ppl: 98.8776  Val Loss: 4.5939\n",
      "Epoch: 1/10 Step: 512... Loss: 4.5132... ppl: 94.9554  Val Loss: 4.5534\n",
      "Epoch: 1/10 Step: 544... Loss: 4.6698... ppl: 91.2204  Val Loss: 4.5133\n",
      "Epoch: 1/10 Step: 576... Loss: 5.6071... ppl: 88.6925  Val Loss: 4.4852\n",
      "Epoch: 1/10 Step: 608... Loss: 4.4119... ppl: 86.2750  Val Loss: 4.4575\n",
      "Epoch: 1/10 Step: 640... Loss: 4.4272... ppl: 83.3755  Val Loss: 4.4234\n",
      "Epoch: 1/10 Step: 672... Loss: 4.4671... ppl: 81.4994  Val Loss: 4.4006\n",
      "Epoch: 1/10 Step: 704... Loss: 4.2286... ppl: 79.2242  Val Loss: 4.3723\n",
      "Epoch: 1/10 Step: 736... Loss: 4.7915... ppl: 77.3133  Val Loss: 4.3479\n",
      "Epoch: 1/10 Step: 768... Loss: 3.8147... ppl: 75.2741  Val Loss: 4.3211\n",
      "Epoch: 1/10 Step: 800... Loss: 4.1720... ppl: 73.7106  Val Loss: 4.3001\n",
      "Epoch: 1/10 Step: 832... Loss: 4.6817... ppl: 72.4208  Val Loss: 4.2825\n",
      "Epoch: 1/10 Step: 864... Loss: 4.1069... ppl: 71.1548  Val Loss: 4.2649\n",
      "Epoch: 1/10 Step: 896... Loss: 4.3370... ppl: 69.8040  Val Loss: 4.2457\n",
      "Epoch: 1/10 Step: 928... Loss: 4.3276... ppl: 67.9632  Val Loss: 4.2190\n",
      "Epoch: 1/10 Step: 960... Loss: 4.0232... ppl: 67.5014  Val Loss: 4.2121\n",
      "Epoch: 1/10 Step: 992... Loss: 3.8108... ppl: 65.7408  Val Loss: 4.1857\n",
      "Epoch: 1/10 Step: 1024... Loss: 3.6372... ppl: 64.6234  Val Loss: 4.1686\n",
      "Epoch: 1/10 Step: 1056... Loss: 4.2216... ppl: 63.5832  Val Loss: 4.1523\n",
      "Epoch: 1/10 Step: 1088... Loss: 3.4432... ppl: 62.3569  Val Loss: 4.1329\n",
      "Epoch: 1/10 Step: 1120... Loss: 4.1201... ppl: 61.6301  Val Loss: 4.1212\n",
      "Epoch: 1/10 Step: 1152... Loss: 3.1725... ppl: 60.8625  Val Loss: 4.1086\n",
      "Epoch: 1/10 Step: 1184... Loss: 4.0171... ppl: 59.8590  Val Loss: 4.0920\n",
      "Epoch: 1/10 Step: 1216... Loss: 3.7921... ppl: 59.2212  Val Loss: 4.0813\n",
      "Epoch: 1/10 Step: 1248... Loss: 4.1599... ppl: 58.3058  Val Loss: 4.0657\n",
      "Epoch: 1/10 Step: 1280... Loss: 4.2683... ppl: 57.5618  Val Loss: 4.0529\n",
      "Epoch: 1/10 Step: 1312... Loss: 4.8147... ppl: 57.3757  Val Loss: 4.0496\n",
      "Epoch: 1/10 Step: 1344... Loss: 3.8214... ppl: 56.7009  Val Loss: 4.0378\n",
      "Epoch: 1/10 Step: 1376... Loss: 4.1512... ppl: 55.6045  Val Loss: 4.0183\n",
      "Epoch: 1/10 Step: 1408... Loss: 3.5986... ppl: 54.8285  Val Loss: 4.0042\n",
      "Epoch: 1/10 Step: 1440... Loss: 4.1593... ppl: 54.4081  Val Loss: 3.9965\n",
      "Epoch: 1/10 Step: 1472... Loss: 3.4239... ppl: 54.0516  Val Loss: 3.9899\n",
      "Epoch: 1/10 Step: 1504... Loss: 4.1077... ppl: 53.7399  Val Loss: 3.9842\n",
      "Epoch: 1/10 Step: 1536... Loss: 4.7577... ppl: 53.3491  Val Loss: 3.9769\n",
      "Epoch: 1/10 Step: 1568... Loss: 4.0043... ppl: 52.1553  Val Loss: 3.9542\n",
      "Epoch: 1/10 Step: 1600... Loss: 3.7289... ppl: 51.6707  Val Loss: 3.9449\n",
      "Epoch: 1/10 Step: 1632... Loss: 5.1503... ppl: 51.0648  Val Loss: 3.9331\n",
      "Epoch: 1/10 Step: 1664... Loss: 4.0014... ppl: 50.6539  Val Loss: 3.9250\n",
      "Epoch: 1/10 Step: 1696... Loss: 3.5989... ppl: 50.3076  Val Loss: 3.9182\n",
      "Epoch: 1/10 Step: 1728... Loss: 3.6451... ppl: 49.9133  Val Loss: 3.9103\n",
      "Epoch: 1/10 Step: 1760... Loss: 4.4726... ppl: 49.8569  Val Loss: 3.9092\n",
      "Epoch: 1/10 Step: 1792... Loss: 3.5054... ppl: 49.2770  Val Loss: 3.8975\n",
      "Epoch: 1/10 Step: 1824... Loss: 4.3797... ppl: 49.5868  Val Loss: 3.9037\n",
      "Epoch: 1/10 Step: 1856... Loss: 5.6778... ppl: 48.3759  Val Loss: 3.8790\n",
      "Epoch: 1/10 Step: 1888... Loss: 3.6859... ppl: 47.8731  Val Loss: 3.8686\n",
      "Epoch: 1/10 Step: 1920... Loss: 3.4105... ppl: 47.5304  Val Loss: 3.8614\n",
      "Epoch: 1/10 Step: 1952... Loss: 3.0092... ppl: 47.1733  Val Loss: 3.8538\n",
      "Epoch: 1/10 Step: 1984... Loss: 2.8947... ppl: 47.1835  Val Loss: 3.8540\n",
      "Epoch: 1/10 Step: 2016... Loss: 3.5369... ppl: 46.4269  Val Loss: 3.8379\n",
      "Epoch: 1/10 Step: 2048... Loss: 4.0542... ppl: 46.2304  Val Loss: 3.8336\n",
      "Epoch: 1/10 Step: 2080... Loss: 3.7860... ppl: 46.0400  Val Loss: 3.8295\n",
      "Epoch: 1/10 Step: 2112... Loss: 4.6811... ppl: 46.0091  Val Loss: 3.8288\n",
      "Epoch: 1/10 Step: 2144... Loss: 3.4756... ppl: 45.5146  Val Loss: 3.8180\n",
      "Epoch: 1/10 Step: 2176... Loss: 5.1243... ppl: 45.3205  Val Loss: 3.8138\n",
      "Epoch: 1/10 Step: 2208... Loss: 4.1630... ppl: 45.2928  Val Loss: 3.8131\n",
      "Epoch: 1/10 Step: 2240... Loss: 3.6505... ppl: 44.9933  Val Loss: 3.8065\n",
      "Epoch: 1/10 Step: 2272... Loss: 3.9523... ppl: 44.4455  Val Loss: 3.7943\n",
      "Epoch: 1/10 Step: 2304... Loss: 3.2948... ppl: 44.0478  Val Loss: 3.7853\n",
      "Epoch: 1/10 Step: 2336... Loss: 4.2125... ppl: 43.9062  Val Loss: 3.7821\n",
      "Epoch: 2/10 Step: 2368... Loss: 3.1954... ppl: 44.1213  Val Loss: 3.7869\n",
      "Epoch: 2/10 Step: 2400... Loss: 3.4278... ppl: 43.8539  Val Loss: 3.7809\n",
      "Epoch: 2/10 Step: 2432... Loss: 3.8476... ppl: 44.1119  Val Loss: 3.7867\n",
      "Epoch: 2/10 Step: 2464... Loss: 4.1759... ppl: 43.9257  Val Loss: 3.7825\n",
      "Epoch: 2/10 Step: 2496... Loss: 3.9813... ppl: 43.3653  Val Loss: 3.7697\n",
      "Epoch: 2/10 Step: 2528... Loss: 3.5283... ppl: 43.1480  Val Loss: 3.7646\n",
      "Epoch: 2/10 Step: 2560... Loss: 5.2517... ppl: 42.9553  Val Loss: 3.7602\n",
      "Epoch: 2/10 Step: 2592... Loss: 4.0514... ppl: 43.0173  Val Loss: 3.7616\n",
      "Epoch: 2/10 Step: 2624... Loss: 3.1987... ppl: 42.7583  Val Loss: 3.7556\n",
      "Epoch: 2/10 Step: 2656... Loss: 3.8203... ppl: 42.6416  Val Loss: 3.7528\n",
      "Epoch: 2/10 Step: 2688... Loss: 3.3859... ppl: 42.2181  Val Loss: 3.7428\n",
      "Epoch: 2/10 Step: 2720... Loss: 3.7371... ppl: 41.9653  Val Loss: 3.7368\n",
      "Epoch: 2/10 Step: 2752... Loss: 3.6380... ppl: 42.0363  Val Loss: 3.7385\n",
      "Epoch: 2/10 Step: 2784... Loss: 4.1078... ppl: 42.0846  Val Loss: 3.7397\n",
      "Epoch: 2/10 Step: 2816... Loss: 4.0771... ppl: 42.0918  Val Loss: 3.7399\n",
      "Epoch: 2/10 Step: 2848... Loss: 3.5282... ppl: 41.5567  Val Loss: 3.7271\n",
      "Epoch: 2/10 Step: 2880... Loss: 4.2115... ppl: 41.4853  Val Loss: 3.7253\n",
      "Epoch: 2/10 Step: 2912... Loss: 3.2448... ppl: 41.1088  Val Loss: 3.7162\n",
      "Epoch: 2/10 Step: 2944... Loss: 3.3571... ppl: 41.3101  Val Loss: 3.7211\n",
      "Epoch: 2/10 Step: 2976... Loss: 3.3597... ppl: 40.9420  Val Loss: 3.7122\n",
      "Epoch: 2/10 Step: 3008... Loss: 4.1141... ppl: 40.8043  Val Loss: 3.7088\n",
      "Epoch: 2/10 Step: 3040... Loss: 3.4985... ppl: 40.7836  Val Loss: 3.7083\n",
      "Epoch: 2/10 Step: 3072... Loss: 3.6767... ppl: 40.5790  Val Loss: 3.7033\n",
      "Epoch: 2/10 Step: 3104... Loss: 3.8094... ppl: 40.5494  Val Loss: 3.7025\n",
      "Epoch: 2/10 Step: 3136... Loss: 4.1361... ppl: 40.3342  Val Loss: 3.6972\n",
      "Epoch: 2/10 Step: 3168... Loss: 3.4341... ppl: 40.2340  Val Loss: 3.6947\n",
      "Epoch: 2/10 Step: 3200... Loss: 3.7930... ppl: 40.3797  Val Loss: 3.6983\n",
      "Epoch: 2/10 Step: 3232... Loss: 4.2722... ppl: 40.1060  Val Loss: 3.6915\n",
      "Epoch: 2/10 Step: 3264... Loss: 2.7940... ppl: 39.7149  Val Loss: 3.6817\n",
      "Epoch: 2/10 Step: 3296... Loss: 4.1922... ppl: 39.7565  Val Loss: 3.6828\n",
      "Epoch: 2/10 Step: 3328... Loss: 4.0447... ppl: 39.4707  Val Loss: 3.6756\n",
      "Epoch: 2/10 Step: 3360... Loss: 3.7153... ppl: 39.2243  Val Loss: 3.6693\n",
      "Epoch: 2/10 Step: 3392... Loss: 3.2659... ppl: 39.4047  Val Loss: 3.6739\n",
      "Epoch: 2/10 Step: 3424... Loss: 4.2727... ppl: 39.1475  Val Loss: 3.6673\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(net, epochs, batch_size, lr, print_every, save_path)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available()):\n\u001b[1;32m     56\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mcuda(), targets\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m---> 58\u001b[0m output, val_h \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_h\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m criterion(output, targets\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     61\u001b[0m val_losses\u001b[38;5;241m.\u001b[39mappend(val_loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36mWordLSTM.forward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m     32\u001b[0m embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb_layer(x)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# pass it to LSTM\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m lstm_out, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# pass lstm_out to dropout\u001b[39;00m\n\u001b[1;32m     36\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdroput(lstm_out)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/transformers/lib/python3.8/site-packages/torch/nn/modules/rnn.py:812\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 812\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    815\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    816\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define batch size\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "# train model\n",
    "train(net, batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation: auto-completion system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OS\n",
    "import os\n",
    "\n",
    "# Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version:  1.12.1+cu116\n"
     ]
    }
   ],
   "source": [
    "print(\"Pytorch version: \", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.s3_class import S3Functions\n",
    "\n",
    "s3_funcs = S3Functions(bucket_name='jdgallegoq-text-gen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8e726c91b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reproduce same results\n",
    "SEED = 42\n",
    "# seed on torch\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64776"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "with s3_funcs.read_object('Dailog-dataset.dialogs_dataset') as f:\n",
    "    dialogs = pickle.load(f)\n",
    "len(dialogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ok thank you',\n",
       " 'Thank you for your help',\n",
       " 'Can you tell me the toppings?',\n",
       " 'Thank you, and what is the telephone number?',\n",
       " 'Ok thank you',\n",
       " 'Well I keep hearing a noise when I turn it on',\n",
       " 'I need it as soon as possible',\n",
       " 'no thank you',\n",
       " 'Hi, I need some pizzas?',\n",
       " 'Yes, my family would love that']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print a sample of 10\n",
    "random.sample(dialogs, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text cleaning\n",
    "dialogs_clean = []\n",
    "\n",
    "for i in dialogs:\n",
    "    # remove everything except alphabets\n",
    "    i = re.sub(\"[^a-zA-Z' ]\", \"\", i)\n",
    "    # convert text to lowercase\n",
    "    i = i.lower()\n",
    "    # add cleaned text to the list\n",
    "    dialogs_clean.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i would like to book a ride to the airport',\n",
       " 'well i guess i am going to pass on movies tonight',\n",
       " 'i want uber x i am having a guest so a car is better than a motobike',\n",
       " 'what does the malnati salad have on it',\n",
       " 'can i please get almond milk',\n",
       " 'no thats all ',\n",
       " 'thank you can you send me text me the address of the new theater',\n",
       " 'yes my cell is fine',\n",
       " 'ok let me know',\n",
       " 'nah no room']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print a sample of 10\n",
    "random.sample(dialogs_clean, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# term frequency\n",
    "all_words = \" \".join(dialogs_clean).split()\n",
    "\n",
    "# create dictionary\n",
    "words_dict = {}\n",
    "for word in all_words:\n",
    "    if word in words_dict:\n",
    "        # increment count by 1\n",
    "        words_dict[word] = words_dict[word] + 1\n",
    "    else:\n",
    "        words_dict[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uppermiddle</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shoots</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>geesh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word  count\n",
       "0  uppermiddle      1\n",
       "1       shoots      1\n",
       "2        geesh      1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprare a dataset\n",
    "words_df = pd.DataFrame({\n",
    "    \"word\": list(words_dict.keys()),\n",
    "    \"count\": list(words_dict.values())\n",
    "})\n",
    "# sort\n",
    "words_df = words_df.sort_values(by='count')\n",
    "words_df.reset_index(inplace=True, drop=True)\n",
    "words_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11145</th>\n",
       "      <td>the</td>\n",
       "      <td>15406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11146</th>\n",
       "      <td>i</td>\n",
       "      <td>19654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word  count\n",
       "11145  the  15406\n",
       "11146    i  19654"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11147"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab size\n",
    "len(words_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rare words distribution in the vocabulary: 69.03\n",
      "Rare words coverage in the corpus: 2.27\n"
     ]
    }
   ],
   "source": [
    "# find and replace rare tokens\n",
    "# replace with UNKNOW TOKEN\n",
    "# define rarity threshold\n",
    "rare_thres = 4\n",
    "\n",
    "# get percentage of rare tokens\n",
    "rare_words_count = len(words_df[words_df['count']<rare_thres]['word'])\n",
    "total_word = len(words_df)\n",
    "rare_dist = rare_words_count / total_word\n",
    "\n",
    "rare_cover = words_df[words_df['count'] < rare_thres]['count'].sum()/words_df['count'].sum()\n",
    "\n",
    "print(f\"Rare words distribution in the vocabulary: {rare_dist*100:.2f}\")\n",
    "print(f\"Rare words coverage in the corpus: {rare_cover*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract rare words in a list\n",
    "rare_words = words_df[words_df['count'] < rare_thres]['word'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/64776 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64776/64776 [00:19<00:00, 3252.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# create a pattern with regex\n",
    "pattern = \"\"\n",
    "for i in rare_words:\n",
    "    pattern += \" {} |\".format(i)\n",
    "\n",
    "# remove the last '|' from the pattern\n",
    "pattern = pattern[:-1]\n",
    "\n",
    "# empty list for clean text\n",
    "dialogs_clean_no_rare_words = []\n",
    "for d in tqdm(dialogs_clean):\n",
    "    text = re.sub(pattern, \" <unk> \", d)\n",
    "    dialogs_clean_no_rare_words.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['does it serve traditional chinese dessert',\n",
       " 'how much extra time to reach <unk> ',\n",
       " 'ok lets reserve a table for dinner at hakkasan',\n",
       " 'hello i need to get a car please',\n",
       " 'holiday inn <unk> parkconv <unk> convention center drive <unk> park il',\n",
       " 'bowling alley <unk> highway <unk> park il',\n",
       " 'what types of cars does uber have',\n",
       " \"what's the price difference\",\n",
       " 'ok get me the cheapest please',\n",
       " 'ok then get me the next level']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogs_clean_no_rare_words[520:530]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprare sequences\n",
    "# 1. define sequence lenght based on distribution of sentence lenghts\n",
    "def create_seq(text, seq_lenght=5):\n",
    "    sequence = []\n",
    "    if len(text.split())>seq_lenght:\n",
    "        for i in range(seq_lenght, len(text.split())):\n",
    "            seq = text.split()[i-seq_lenght: i+1]\n",
    "            sequence.append(\" \".join(seq))\n",
    "\n",
    "        return sequence\n",
    "\n",
    "    else:\n",
    "        return [text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = [create_seq(i) for i in dialogs_clean_no_rare_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"hi i'm looking to book a\",\n",
       "  \"i'm looking to book a table\",\n",
       "  'looking to book a table for',\n",
       "  'to book a table for korean',\n",
       "  'book a table for korean fod'],\n",
       " ['somewhere in southern nyc maybe the',\n",
       "  'in southern nyc maybe the east',\n",
       "  'southern nyc maybe the east village'],\n",
       " [\"we don't want to sit at\",\n",
       "  \"don't want to sit at the\",\n",
       "  'want to sit at the bar',\n",
       "  'to sit at the bar but',\n",
       "  'sit at the bar but anywhere',\n",
       "  'at the bar but anywhere else',\n",
       "  'the bar but anywhere else is',\n",
       "  'bar but anywhere else is fine'],\n",
       " ['what times are available'],\n",
       " [\"yikes we can't do those times\"],\n",
       " ['let me check'],\n",
       " [\"great let's book that\"],\n",
       " [\"no that's it just book\"],\n",
       " ['hi i would like to see',\n",
       "  'i would like to see if',\n",
       "  'would like to see if the',\n",
       "  'like to see if the movie',\n",
       "  'to see if the movie what',\n",
       "  'see if the movie what men',\n",
       "  'if the movie what men want',\n",
       "  'the movie what men want is',\n",
       "  'movie what men want is playing',\n",
       "  'what men want is playing here'],\n",
       " ['yes for me and a friend',\n",
       "  'for me and a friend so',\n",
       "  'me and a friend so two',\n",
       "  'and a friend so two tickets',\n",
       "  'a friend so two tickets please']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"hi i'm looking to book a\",\n",
       " \"i'm looking to book a table\",\n",
       " 'looking to book a table for',\n",
       " 'to book a table for korean',\n",
       " 'book a table for korean fod',\n",
       " 'somewhere in southern nyc maybe the',\n",
       " 'in southern nyc maybe the east',\n",
       " 'southern nyc maybe the east village',\n",
       " \"we don't want to sit at\",\n",
       " \"don't want to sit at the\",\n",
       " 'want to sit at the bar',\n",
       " 'to sit at the bar but',\n",
       " 'sit at the bar but anywhere',\n",
       " 'at the bar but anywhere else',\n",
       " 'the bar but anywhere else is']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge to a single list\n",
    "seqs = sum(seqs, [])\n",
    "seqs[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205346"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count sequences\n",
    "len(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"hi i'm looking to book\", \"i'm looking to book a\")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create input and target sequences (x and y)\n",
    "x = []\n",
    "y = []\n",
    "for s in seqs:\n",
    "    x.append(\" \".join(s.split()[:-1]))\n",
    "    y.append(\" \".join(s.split()[1:]))\n",
    "\n",
    "x[0], y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5148, \"children's\")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create token-integer mappings\n",
    "# text-gen works like generation integers\n",
    "# that need to be converted into tokens by mapping\n",
    "# the vocabulary\n",
    "\n",
    "int2token = {}\n",
    "count = 1\n",
    "\n",
    "for w in set(\" \".join(dialogs_clean_no_rare_words).split()):\n",
    "    int2token[count] = w\n",
    "    count += 1\n",
    "\n",
    "# token to integer mapping (inverse of above)\n",
    "token2int = {v: k for k, v in int2token.items()}\n",
    "\n",
    "token2int['can'], int2token[1127]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_thres = 150000\n",
    "\n",
    "x_train = x[:split_thres]\n",
    "x_val = x[split_thres:]\n",
    "y_train = y[:split_thres]\n",
    "y_val = y[split_thres:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad sequences\n",
    "Explore sequence lenghts to get the optimal padding lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(i.split()) for i in x_train])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max lenght should be 5\n",
    "max_text_len = 5\n",
    "\n",
    "def pad_sequences(seq, n):\n",
    "    # get tokens from seq\n",
    "    seq = seq.split()\n",
    "    # check if seq len < n\n",
    "    if len(seq)<n:\n",
    "        for i in range(n-len(seq)):\n",
    "            seq.append(\"<pad>\")\n",
    "    return \" \".join(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad sequences\n",
    "x_train_pad = [pad_sequences(s, max_text_len) for s in x_train]\n",
    "y_train_pad = [pad_sequences(s, max_text_len) for s in y_train]\n",
    "\n",
    "x_val_pad = [pad_sequences(s, max_text_len) for s in x_val]\n",
    "y_val_pad = [pad_sequences(s, max_text_len) for s in y_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"hi i'm looking to book\",\n",
       " \"i'm looking to book a\",\n",
       " 'looking to book a table',\n",
       " 'to book a table for',\n",
       " 'book a table for korean',\n",
       " 'somewhere in southern nyc maybe',\n",
       " 'in southern nyc maybe the',\n",
       " 'southern nyc maybe the east',\n",
       " \"we don't want to sit\",\n",
       " \"don't want to sit at\",\n",
       " 'want to sit at the',\n",
       " 'to sit at the bar',\n",
       " 'sit at the bar but',\n",
       " 'at the bar but anywhere',\n",
       " 'the bar but anywhere else',\n",
       " 'bar but anywhere else is',\n",
       " 'what times are <pad> <pad>',\n",
       " \"yikes we can't do those\",\n",
       " 'let me <pad> <pad> <pad>',\n",
       " \"great let's book <pad> <pad>\"]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_pad[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"i'm looking to book a\",\n",
       " 'looking to book a table',\n",
       " 'to book a table for',\n",
       " 'book a table for korean',\n",
       " 'a table for korean fod',\n",
       " 'in southern nyc maybe the',\n",
       " 'southern nyc maybe the east',\n",
       " 'nyc maybe the east village',\n",
       " \"don't want to sit at\",\n",
       " 'want to sit at the',\n",
       " 'to sit at the bar',\n",
       " 'sit at the bar but',\n",
       " 'at the bar but anywhere',\n",
       " 'the bar but anywhere else',\n",
       " 'bar but anywhere else is',\n",
       " 'but anywhere else is fine',\n",
       " 'times are available <pad> <pad>',\n",
       " \"we can't do those times\",\n",
       " 'me check <pad> <pad> <pad>',\n",
       " \"let's book that <pad> <pad>\"]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pad[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update vocab\n",
    "int2token[0] = \"<pad>\"\n",
    "token2int[\"<pad>\"] = 0\n",
    "# get vocab size\n",
    "vocab_size = len(int2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert padded sequences to integers\n",
    "def get_integer_seq(seq):\n",
    "    return [token2int[w] for w in seq.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply above function to x and y\n",
    "x_train_int = [get_integer_seq(s) for s in x_train_pad]\n",
    "y_train_int = [get_integer_seq(s) for s in y_train_pad]\n",
    "x_val_int = [get_integer_seq(s) for s in x_val_pad]\n",
    "y_val_int = [get_integer_seq(s) for s in y_val_pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3619, 5551, 1328, 1337, 3731],\n",
       " [5551, 1328, 1337, 3731, 5048],\n",
       " [1328, 1337, 3731, 5048, 891],\n",
       " [1337, 3731, 5048, 891, 738],\n",
       " [3731, 5048, 891, 738, 1462],\n",
       " [4936, 759, 4992, 5256, 2118],\n",
       " [759, 4992, 5256, 2118, 4624],\n",
       " [4992, 5256, 2118, 4624, 4969],\n",
       " [1576, 4531, 2751, 1337, 4318],\n",
       " [4531, 2751, 1337, 4318, 2675]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_int[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5551, 1328, 1337, 3731, 5048],\n",
       " [1328, 1337, 3731, 5048, 891],\n",
       " [1337, 3731, 5048, 891, 738],\n",
       " [3731, 5048, 891, 738, 1462],\n",
       " [5048, 891, 738, 1462, 5341],\n",
       " [759, 4992, 5256, 2118, 4624],\n",
       " [4992, 5256, 2118, 4624, 4969],\n",
       " [5256, 2118, 4624, 4969, 2045],\n",
       " [4531, 2751, 1337, 4318, 2675],\n",
       " [2751, 1337, 4318, 2675, 4624]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_int[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150000, 5), (150000, 5), (55346, 5), (55346, 5))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert lists into arrays\n",
    "x_train_int = np.array(x_train_int)\n",
    "y_train_int = np.array(y_train_int)\n",
    "x_val_int = np.array(x_val_int)\n",
    "y_val_int = np.array(y_val_int)\n",
    "\n",
    "x_train_int.shape, y_train_int.shape, x_val_int.shape, y_val_int.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "-> embedding layer:</br>\n",
    "    - input dim: vocab size</br>\n",
    "    - output dim: 200 (hyperparameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordLSTM(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_hidden=256,\n",
    "            n_layers=2,\n",
    "            drop_prob=0.3,\n",
    "            lr=1e-3\n",
    "        ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "\n",
    "        self.emb_layer = nn.Embedding(vocab_size, 200)\n",
    "\n",
    "        # define LSTM\n",
    "        # as input shape is (batch size, seq len, num features)\n",
    "        # set batch_first=True\n",
    "        self.lstm = nn.LSTM(200, n_hidden, n_layers, batch_first=True)\n",
    "\n",
    "        # Droput layer\n",
    "        self.droput = nn.Dropout(drop_prob)\n",
    "\n",
    "        # Dense layer\n",
    "        self.fc = nn.Linear(n_hidden, vocab_size)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        # get embed layer\n",
    "        embedded = self.emb_layer(x)\n",
    "        # pass it to LSTM\n",
    "        lstm_out, hidden = self.lstm(embedded, hidden)\n",
    "        # pass lstm_out to dropout\n",
    "        out = self.droput(lstm_out)\n",
    "        # reshape (batch size*seq_len, hidden_units)\n",
    "        out = out.reshape(-1, self.n_hidden)\n",
    "        # pass out to dense\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "            initialize first hidden and cell state for LSTM\n",
    "        \"\"\"\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        if (torch.cuda.is_available()):\n",
    "            hidden = (\n",
    "                # this first one for the hidden state\n",
    "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                # this second one for the cell state\n",
    "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda()\n",
    "            )\n",
    "        else:\n",
    "            hidden = (\n",
    "                # this first one for the hidden state\n",
    "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                # this second one for the cell state\n",
    "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_()\n",
    "            )\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordLSTM(\n",
      "  (emb_layer): Embedding(6502, 200)\n",
      "  (lstm): LSTM(200, 256, num_layers=2, batch_first=True)\n",
      "  (droput): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=6502, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# summary\n",
    "net = WordLSTM()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to pass batches\n",
    "def get_batches(x_array, y_array, batch_size):\n",
    "    prv = 0\n",
    "    for n in range(batch_size, x_array.shape[0], batch_size):\n",
    "        # batch for input\n",
    "        x = x_array[prv:n, :]\n",
    "        # batch for target\n",
    "        y = y_array[prv:n, :]\n",
    "        prv = n\n",
    "\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    net,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    lr=1e-3,\n",
    "    print_every=32,\n",
    "    save_path='../module/models'\n",
    "):\n",
    "    # set initial loss to infinite\n",
    "    best_valid_loss = float('inf')\n",
    "    # optimizer\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    # loss funtion\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # if GPU available push model to GPU\n",
    "    if (torch.cuda.is_available()):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    net.train()\n",
    "    for e in range(epochs):\n",
    "        # iterate over batches\n",
    "        for x, y in get_batches(x_train_int, y_train_int, batch_size):\n",
    "            counter += 1\n",
    "            # convert arrays to tensors\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            # if GPU available then push tensors to GPU\n",
    "            if (torch.cuda.is_available()):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            \n",
    "            # initialize hidden state\n",
    "            # this prevents to carry on information from\n",
    "            # previous batches\n",
    "            h = net.init_hidden(batch_size)\n",
    "            # set accumulated gradients to zero\n",
    "            net.zero_grad()\n",
    "            # get output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            # calculate loss and do backward\n",
    "            loss = criterion(output, targets.view(-1))\n",
    "            loss.backward()\n",
    "            # optimier step\n",
    "            opt.step()\n",
    "\n",
    "            if counter % print_every == 0:\n",
    "                # get val loss\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "\n",
    "                for x, y in get_batches(x_val_int, y_val_int, batch_size):\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    val_h = net.init_hidden(batch_size)\n",
    "\n",
    "                    inputs, targets = x, y\n",
    "                    if (torch.cuda.is_available()):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                    \n",
    "                    output, val_h = net(inputs, val_h)\n",
    "\n",
    "                    val_loss = criterion(output, targets.view(-1))\n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                # save best model\n",
    "                if np.mean(val_losses) < best_valid_loss:\n",
    "                    best_valid_loss = np.mean(val_losses)\n",
    "                    torch.save(net.state_dict(), f'{save_path}/saved_weights.pt')\n",
    "                \n",
    "                net.train()\n",
    "\n",
    "                print(\n",
    "                    \"Epoch: {}/{}\".format(e+1, epochs),\n",
    "                    \"Step: {}...\".format(counter),\n",
    "                    \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                    \"ppl: {:.4f} \".format(np.exp(np.mean(val_losses))),\n",
    "                    \"Val Loss: {:.4f}\".format(np.mean(val_losses))\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 Step: 32... Loss: 6.4566... ppl: 687.1249  Val Loss: 6.5325\n",
      "Epoch: 1/10 Step: 64... Loss: 5.7009... ppl: 342.4059  Val Loss: 5.8360\n",
      "Epoch: 1/10 Step: 96... Loss: 6.0428... ppl: 271.9060  Val Loss: 5.6055\n",
      "Epoch: 1/10 Step: 128... Loss: 5.2416... ppl: 237.9236  Val Loss: 5.4719\n",
      "Epoch: 1/10 Step: 160... Loss: 6.2091... ppl: 211.0929  Val Loss: 5.3523\n",
      "Epoch: 1/10 Step: 192... Loss: 5.7195... ppl: 188.6304  Val Loss: 5.2398\n",
      "Epoch: 1/10 Step: 224... Loss: 5.1258... ppl: 172.6018  Val Loss: 5.1510\n",
      "Epoch: 1/10 Step: 256... Loss: 4.3225... ppl: 158.2669  Val Loss: 5.0643\n",
      "Epoch: 1/10 Step: 288... Loss: 4.9426... ppl: 145.5531  Val Loss: 4.9805\n",
      "Epoch: 1/10 Step: 320... Loss: 5.0588... ppl: 135.9070  Val Loss: 4.9120\n",
      "Epoch: 1/10 Step: 352... Loss: 4.3914... ppl: 126.5371  Val Loss: 4.8405\n",
      "Epoch: 1/10 Step: 384... Loss: 5.0741... ppl: 119.1219  Val Loss: 4.7801\n",
      "Epoch: 1/10 Step: 416... Loss: 4.8709... ppl: 112.5293  Val Loss: 4.7232\n",
      "Epoch: 1/10 Step: 448... Loss: 4.5387... ppl: 107.2491  Val Loss: 4.6752\n",
      "Epoch: 1/10 Step: 480... Loss: 4.3547... ppl: 102.8325  Val Loss: 4.6331\n",
      "Epoch: 1/10 Step: 512... Loss: 4.4990... ppl: 98.7199  Val Loss: 4.5923\n",
      "Epoch: 1/10 Step: 544... Loss: 4.7472... ppl: 95.0527  Val Loss: 4.5544\n",
      "Epoch: 1/10 Step: 576... Loss: 5.5489... ppl: 91.7163  Val Loss: 4.5187\n",
      "Epoch: 1/10 Step: 608... Loss: 4.5208... ppl: 89.3552  Val Loss: 4.4926\n",
      "Epoch: 1/10 Step: 640... Loss: 4.5040... ppl: 86.0387  Val Loss: 4.4548\n",
      "Epoch: 1/10 Step: 672... Loss: 4.4175... ppl: 84.0957  Val Loss: 4.4320\n",
      "Epoch: 1/10 Step: 704... Loss: 4.2498... ppl: 81.8134  Val Loss: 4.4044\n",
      "Epoch: 1/10 Step: 736... Loss: 4.7607... ppl: 80.1592  Val Loss: 4.3840\n",
      "Epoch: 1/10 Step: 768... Loss: 3.8979... ppl: 77.7901  Val Loss: 4.3540\n",
      "Epoch: 1/10 Step: 800... Loss: 4.1924... ppl: 75.9113  Val Loss: 4.3296\n",
      "Epoch: 1/10 Step: 832... Loss: 4.7261... ppl: 74.3908  Val Loss: 4.3093\n",
      "Epoch: 1/10 Step: 864... Loss: 4.1090... ppl: 73.1722  Val Loss: 4.2928\n",
      "Epoch: 1/10 Step: 896... Loss: 4.3646... ppl: 71.8364  Val Loss: 4.2744\n",
      "Epoch: 1/10 Step: 928... Loss: 4.3902... ppl: 69.8496  Val Loss: 4.2463\n",
      "Epoch: 1/10 Step: 960... Loss: 3.9573... ppl: 69.3614  Val Loss: 4.2393\n",
      "Epoch: 1/10 Step: 992... Loss: 3.8208... ppl: 67.5967  Val Loss: 4.2136\n",
      "Epoch: 1/10 Step: 1024... Loss: 3.6925... ppl: 66.3501  Val Loss: 4.1949\n",
      "Epoch: 1/10 Step: 1056... Loss: 4.2531... ppl: 65.1358  Val Loss: 4.1765\n",
      "Epoch: 1/10 Step: 1088... Loss: 3.4417... ppl: 64.0472  Val Loss: 4.1596\n",
      "Epoch: 1/10 Step: 1120... Loss: 4.2096... ppl: 63.3622  Val Loss: 4.1489\n",
      "Epoch: 1/10 Step: 1152... Loss: 3.2568... ppl: 62.5542  Val Loss: 4.1360\n",
      "Epoch: 1/10 Step: 1184... Loss: 3.9805... ppl: 61.6429  Val Loss: 4.1214\n",
      "Epoch: 1/10 Step: 1216... Loss: 3.8156... ppl: 60.8942  Val Loss: 4.1091\n",
      "Epoch: 1/10 Step: 1248... Loss: 4.1482... ppl: 59.8072  Val Loss: 4.0911\n",
      "Epoch: 1/10 Step: 1280... Loss: 4.2864... ppl: 58.9056  Val Loss: 4.0759\n",
      "Epoch: 1/10 Step: 1312... Loss: 4.8609... ppl: 58.5853  Val Loss: 4.0705\n",
      "Epoch: 1/10 Step: 1344... Loss: 3.8113... ppl: 57.7868  Val Loss: 4.0568\n",
      "Epoch: 1/10 Step: 1376... Loss: 4.1013... ppl: 56.7208  Val Loss: 4.0381\n",
      "Epoch: 1/10 Step: 1408... Loss: 3.6077... ppl: 56.0688  Val Loss: 4.0266\n",
      "Epoch: 1/10 Step: 1440... Loss: 4.1507... ppl: 55.3493  Val Loss: 4.0137\n",
      "Epoch: 1/10 Step: 1472... Loss: 3.5246... ppl: 54.9079  Val Loss: 4.0057\n",
      "Epoch: 1/10 Step: 1504... Loss: 4.2427... ppl: 54.6562  Val Loss: 4.0011\n",
      "Epoch: 1/10 Step: 1536... Loss: 4.9114... ppl: 54.2917  Val Loss: 3.9944\n",
      "Epoch: 1/10 Step: 1568... Loss: 3.9939... ppl: 53.1677  Val Loss: 3.9735\n",
      "Epoch: 1/10 Step: 1600... Loss: 3.7048... ppl: 52.7481  Val Loss: 3.9655\n",
      "Epoch: 1/10 Step: 1632... Loss: 5.2662... ppl: 52.0086  Val Loss: 3.9514\n",
      "Epoch: 1/10 Step: 1664... Loss: 3.9827... ppl: 51.5238  Val Loss: 3.9420\n",
      "Epoch: 1/10 Step: 1696... Loss: 3.6302... ppl: 51.0669  Val Loss: 3.9331\n",
      "Epoch: 1/10 Step: 1728... Loss: 3.7046... ppl: 50.6395  Val Loss: 3.9247\n",
      "Epoch: 1/10 Step: 1760... Loss: 4.6003... ppl: 50.6843  Val Loss: 3.9256\n",
      "Epoch: 1/10 Step: 1792... Loss: 3.4384... ppl: 50.0463  Val Loss: 3.9129\n",
      "Epoch: 1/10 Step: 1824... Loss: 4.3520... ppl: 50.1134  Val Loss: 3.9143\n",
      "Epoch: 1/10 Step: 1856... Loss: 5.7410... ppl: 49.2783  Val Loss: 3.8975\n",
      "Epoch: 1/10 Step: 1888... Loss: 3.6659... ppl: 48.3939  Val Loss: 3.8794\n",
      "Epoch: 1/10 Step: 1920... Loss: 3.5246... ppl: 48.0683  Val Loss: 3.8726\n",
      "Epoch: 1/10 Step: 1952... Loss: 3.0443... ppl: 47.8120  Val Loss: 3.8673\n",
      "Epoch: 1/10 Step: 1984... Loss: 2.9488... ppl: 47.7416  Val Loss: 3.8658\n",
      "Epoch: 1/10 Step: 2016... Loss: 3.5612... ppl: 47.0555  Val Loss: 3.8513\n",
      "Epoch: 1/10 Step: 2048... Loss: 4.0600... ppl: 46.7752  Val Loss: 3.8454\n",
      "Epoch: 1/10 Step: 2080... Loss: 3.8548... ppl: 46.5419  Val Loss: 3.8404\n",
      "Epoch: 1/10 Step: 2112... Loss: 4.8001... ppl: 46.4009  Val Loss: 3.8373\n",
      "Epoch: 1/10 Step: 2144... Loss: 3.5676... ppl: 46.0290  Val Loss: 3.8293\n",
      "Epoch: 1/10 Step: 2176... Loss: 5.1230... ppl: 45.6475  Val Loss: 3.8209\n",
      "Epoch: 1/10 Step: 2208... Loss: 4.1744... ppl: 45.7058  Val Loss: 3.8222\n",
      "Epoch: 1/10 Step: 2240... Loss: 3.5958... ppl: 45.5456  Val Loss: 3.8187\n",
      "Epoch: 1/10 Step: 2272... Loss: 3.9769... ppl: 44.8076  Val Loss: 3.8024\n",
      "Epoch: 1/10 Step: 2304... Loss: 3.2873... ppl: 44.4503  Val Loss: 3.7944\n",
      "Epoch: 1/10 Step: 2336... Loss: 4.3177... ppl: 44.3152  Val Loss: 3.7913\n",
      "Epoch: 2/10 Step: 2368... Loss: 3.2370... ppl: 44.3341  Val Loss: 3.7918\n",
      "Epoch: 2/10 Step: 2400... Loss: 3.3761... ppl: 44.2202  Val Loss: 3.7892\n",
      "Epoch: 2/10 Step: 2432... Loss: 3.8272... ppl: 44.5850  Val Loss: 3.7974\n",
      "Epoch: 2/10 Step: 2464... Loss: 4.2594... ppl: 44.1447  Val Loss: 3.7875\n",
      "Epoch: 2/10 Step: 2496... Loss: 4.0042... ppl: 43.8453  Val Loss: 3.7807\n",
      "Epoch: 2/10 Step: 2528... Loss: 3.5174... ppl: 43.4662  Val Loss: 3.7720\n",
      "Epoch: 2/10 Step: 2560... Loss: 5.2677... ppl: 43.1497  Val Loss: 3.7647\n",
      "Epoch: 2/10 Step: 2592... Loss: 4.0560... ppl: 43.1842  Val Loss: 3.7655\n",
      "Epoch: 2/10 Step: 2624... Loss: 3.1494... ppl: 43.0188  Val Loss: 3.7616\n",
      "Epoch: 2/10 Step: 2656... Loss: 3.8433... ppl: 42.8354  Val Loss: 3.7574\n",
      "Epoch: 2/10 Step: 2688... Loss: 3.3461... ppl: 42.5312  Val Loss: 3.7502\n",
      "Epoch: 2/10 Step: 2720... Loss: 3.7294... ppl: 42.2306  Val Loss: 3.7431\n",
      "Epoch: 2/10 Step: 2752... Loss: 3.6309... ppl: 42.2440  Val Loss: 3.7435\n",
      "Epoch: 2/10 Step: 2784... Loss: 4.2614... ppl: 42.1106  Val Loss: 3.7403\n",
      "Epoch: 2/10 Step: 2816... Loss: 4.0102... ppl: 42.0379  Val Loss: 3.7386\n",
      "Epoch: 2/10 Step: 2848... Loss: 3.6342... ppl: 41.6979  Val Loss: 3.7304\n",
      "Epoch: 2/10 Step: 2880... Loss: 4.1393... ppl: 41.7052  Val Loss: 3.7306\n",
      "Epoch: 2/10 Step: 2912... Loss: 3.2426... ppl: 41.3875  Val Loss: 3.7230\n",
      "Epoch: 2/10 Step: 2944... Loss: 3.4054... ppl: 41.3756  Val Loss: 3.7227\n",
      "Epoch: 2/10 Step: 2976... Loss: 3.3263... ppl: 40.9819  Val Loss: 3.7131\n",
      "Epoch: 2/10 Step: 3008... Loss: 4.1029... ppl: 41.0826  Val Loss: 3.7156\n",
      "Epoch: 2/10 Step: 3040... Loss: 3.5420... ppl: 40.8294  Val Loss: 3.7094\n",
      "Epoch: 2/10 Step: 3072... Loss: 3.6546... ppl: 40.6571  Val Loss: 3.7052\n",
      "Epoch: 2/10 Step: 3104... Loss: 3.8362... ppl: 40.5434  Val Loss: 3.7024\n",
      "Epoch: 2/10 Step: 3136... Loss: 4.0336... ppl: 40.4156  Val Loss: 3.6992\n",
      "Epoch: 2/10 Step: 3168... Loss: 3.4537... ppl: 40.3981  Val Loss: 3.6988\n",
      "Epoch: 2/10 Step: 3200... Loss: 3.7745... ppl: 40.5704  Val Loss: 3.7030\n",
      "Epoch: 2/10 Step: 3232... Loss: 4.2158... ppl: 40.0999  Val Loss: 3.6914\n",
      "Epoch: 2/10 Step: 3264... Loss: 2.8377... ppl: 39.8520  Val Loss: 3.6852\n",
      "Epoch: 2/10 Step: 3296... Loss: 4.2166... ppl: 39.8221  Val Loss: 3.6844\n",
      "Epoch: 2/10 Step: 3328... Loss: 4.0922... ppl: 39.6925  Val Loss: 3.6812\n",
      "Epoch: 2/10 Step: 3360... Loss: 3.6523... ppl: 39.3010  Val Loss: 3.6712\n",
      "Epoch: 2/10 Step: 3392... Loss: 3.2306... ppl: 39.5839  Val Loss: 3.6784\n",
      "Epoch: 2/10 Step: 3424... Loss: 4.2662... ppl: 39.1946  Val Loss: 3.6685\n",
      "Epoch: 2/10 Step: 3456... Loss: 3.4374... ppl: 39.0596  Val Loss: 3.6651\n",
      "Epoch: 2/10 Step: 3488... Loss: 3.5640... ppl: 39.3319  Val Loss: 3.6720\n",
      "Epoch: 2/10 Step: 3520... Loss: 3.4280... ppl: 38.8717  Val Loss: 3.6603\n",
      "Epoch: 2/10 Step: 3552... Loss: 4.0293... ppl: 39.0254  Val Loss: 3.6642\n",
      "Epoch: 2/10 Step: 3584... Loss: 2.9670... ppl: 38.7674  Val Loss: 3.6576\n",
      "Epoch: 2/10 Step: 3616... Loss: 3.5423... ppl: 38.6334  Val Loss: 3.6541\n",
      "Epoch: 2/10 Step: 3648... Loss: 3.2321... ppl: 38.6762  Val Loss: 3.6552\n",
      "Epoch: 2/10 Step: 3680... Loss: 3.7557... ppl: 38.5630  Val Loss: 3.6523\n",
      "Epoch: 2/10 Step: 3712... Loss: 3.5057... ppl: 38.4663  Val Loss: 3.6498\n",
      "Epoch: 2/10 Step: 3744... Loss: 3.4137... ppl: 38.1508  Val Loss: 3.6415\n",
      "Epoch: 2/10 Step: 3776... Loss: 3.9275... ppl: 38.2135  Val Loss: 3.6432\n",
      "Epoch: 2/10 Step: 3808... Loss: 4.7996... ppl: 37.9869  Val Loss: 3.6372\n",
      "Epoch: 2/10 Step: 3840... Loss: 4.3512... ppl: 38.2208  Val Loss: 3.6434\n",
      "Epoch: 2/10 Step: 3872... Loss: 4.0453... ppl: 38.0481  Val Loss: 3.6389\n",
      "Epoch: 2/10 Step: 3904... Loss: 2.9896... ppl: 37.8013  Val Loss: 3.6323\n",
      "Epoch: 2/10 Step: 3936... Loss: 3.7858... ppl: 37.8946  Val Loss: 3.6348\n",
      "Epoch: 2/10 Step: 3968... Loss: 3.9768... ppl: 37.5449  Val Loss: 3.6255\n",
      "Epoch: 2/10 Step: 4000... Loss: 3.7906... ppl: 37.5192  Val Loss: 3.6249\n",
      "Epoch: 2/10 Step: 4032... Loss: 4.2110... ppl: 37.4658  Val Loss: 3.6234\n",
      "Epoch: 2/10 Step: 4064... Loss: 3.6387... ppl: 37.3315  Val Loss: 3.6198\n",
      "Epoch: 2/10 Step: 4096... Loss: 3.5527... ppl: 37.4739  Val Loss: 3.6236\n",
      "Epoch: 2/10 Step: 4128... Loss: 2.9084... ppl: 37.4107  Val Loss: 3.6220\n",
      "Epoch: 2/10 Step: 4160... Loss: 2.9362... ppl: 37.4299  Val Loss: 3.6225\n",
      "Epoch: 2/10 Step: 4192... Loss: 3.4284... ppl: 37.1753  Val Loss: 3.6156\n",
      "Epoch: 2/10 Step: 4224... Loss: 3.3201... ppl: 36.7966  Val Loss: 3.6054\n",
      "Epoch: 2/10 Step: 4256... Loss: 3.5031... ppl: 36.9397  Val Loss: 3.6093\n",
      "Epoch: 2/10 Step: 4288... Loss: 3.4673... ppl: 36.7166  Val Loss: 3.6032\n",
      "Epoch: 2/10 Step: 4320... Loss: 2.9601... ppl: 36.6416  Val Loss: 3.6012\n",
      "Epoch: 2/10 Step: 4352... Loss: 3.0386... ppl: 36.6684  Val Loss: 3.6019\n",
      "Epoch: 2/10 Step: 4384... Loss: 3.1621... ppl: 36.5382  Val Loss: 3.5984\n",
      "Epoch: 2/10 Step: 4416... Loss: 3.8633... ppl: 36.5017  Val Loss: 3.5974\n",
      "Epoch: 2/10 Step: 4448... Loss: 2.9812... ppl: 36.6638  Val Loss: 3.6018\n",
      "Epoch: 2/10 Step: 4480... Loss: 4.3472... ppl: 36.4671  Val Loss: 3.5964\n",
      "Epoch: 2/10 Step: 4512... Loss: 3.2859... ppl: 36.3351  Val Loss: 3.5928\n",
      "Epoch: 2/10 Step: 4544... Loss: 3.4654... ppl: 36.3177  Val Loss: 3.5923\n",
      "Epoch: 2/10 Step: 4576... Loss: 2.8299... ppl: 36.3120  Val Loss: 3.5921\n",
      "Epoch: 2/10 Step: 4608... Loss: 2.9031... ppl: 36.3097  Val Loss: 3.5921\n",
      "Epoch: 2/10 Step: 4640... Loss: 3.6808... ppl: 36.1772  Val Loss: 3.5884\n",
      "Epoch: 2/10 Step: 4672... Loss: 3.5216... ppl: 35.9333  Val Loss: 3.5817\n",
      "Epoch: 3/10 Step: 4704... Loss: 3.0874... ppl: 35.9265  Val Loss: 3.5815\n",
      "Epoch: 3/10 Step: 4736... Loss: 4.2820... ppl: 36.1623  Val Loss: 3.5880\n",
      "Epoch: 3/10 Step: 4768... Loss: 3.1047... ppl: 36.2046  Val Loss: 3.5892\n",
      "Epoch: 3/10 Step: 4800... Loss: 2.6744... ppl: 36.2741  Val Loss: 3.5911\n",
      "Epoch: 3/10 Step: 4832... Loss: 3.2731... ppl: 36.1593  Val Loss: 3.5879\n",
      "Epoch: 3/10 Step: 4864... Loss: 3.7137... ppl: 36.1451  Val Loss: 3.5875\n",
      "Epoch: 3/10 Step: 4896... Loss: 2.5786... ppl: 36.0446  Val Loss: 3.5848\n",
      "Epoch: 3/10 Step: 4928... Loss: 2.0622... ppl: 35.9360  Val Loss: 3.5817\n",
      "Epoch: 3/10 Step: 4960... Loss: 3.2481... ppl: 36.0147  Val Loss: 3.5839\n",
      "Epoch: 3/10 Step: 4992... Loss: 3.4663... ppl: 36.2000  Val Loss: 3.5891\n",
      "Epoch: 3/10 Step: 5024... Loss: 3.4507... ppl: 35.8425  Val Loss: 3.5791\n",
      "Epoch: 3/10 Step: 5056... Loss: 3.0707... ppl: 35.7116  Val Loss: 3.5755\n",
      "Epoch: 3/10 Step: 5088... Loss: 2.8119... ppl: 35.6499  Val Loss: 3.5737\n",
      "Epoch: 3/10 Step: 5120... Loss: 3.7405... ppl: 35.7624  Val Loss: 3.5769\n",
      "Epoch: 3/10 Step: 5152... Loss: 2.9842... ppl: 35.9268  Val Loss: 3.5815\n",
      "Epoch: 3/10 Step: 5184... Loss: 2.8694... ppl: 35.8212  Val Loss: 3.5785\n",
      "Epoch: 3/10 Step: 5216... Loss: 2.9320... ppl: 35.4119  Val Loss: 3.5670\n",
      "Epoch: 3/10 Step: 5248... Loss: 3.0404... ppl: 35.4250  Val Loss: 3.5674\n",
      "Epoch: 3/10 Step: 5280... Loss: 3.2178... ppl: 35.6649  Val Loss: 3.5742\n",
      "Epoch: 3/10 Step: 5312... Loss: 3.4895... ppl: 35.4746  Val Loss: 3.5688\n",
      "Epoch: 3/10 Step: 5344... Loss: 3.6292... ppl: 35.4535  Val Loss: 3.5682\n",
      "Epoch: 3/10 Step: 5376... Loss: 4.0982... ppl: 35.4235  Val Loss: 3.5674\n",
      "Epoch: 3/10 Step: 5408... Loss: 3.8915... ppl: 35.4281  Val Loss: 3.5675\n",
      "Epoch: 3/10 Step: 5440... Loss: 4.3496... ppl: 35.5137  Val Loss: 3.5699\n",
      "Epoch: 3/10 Step: 5472... Loss: 3.7555... ppl: 35.1047  Val Loss: 3.5583\n",
      "Epoch: 3/10 Step: 5504... Loss: 3.6309... ppl: 35.2597  Val Loss: 3.5627\n",
      "Epoch: 3/10 Step: 5536... Loss: 3.1613... ppl: 35.5166  Val Loss: 3.5700\n",
      "Epoch: 3/10 Step: 5568... Loss: 3.0836... ppl: 35.2898  Val Loss: 3.5636\n",
      "Epoch: 3/10 Step: 5600... Loss: 3.3630... ppl: 35.1876  Val Loss: 3.5607\n",
      "Epoch: 3/10 Step: 5632... Loss: 3.3564... ppl: 35.1489  Val Loss: 3.5596\n",
      "Epoch: 3/10 Step: 5664... Loss: 3.1959... ppl: 35.1297  Val Loss: 3.5590\n",
      "Epoch: 3/10 Step: 5696... Loss: 3.7199... ppl: 34.9514  Val Loss: 3.5540\n",
      "Epoch: 3/10 Step: 5728... Loss: 3.2789... ppl: 35.3623  Val Loss: 3.5656\n",
      "Epoch: 3/10 Step: 5760... Loss: 3.8210... ppl: 34.9555  Val Loss: 3.5541\n",
      "Epoch: 3/10 Step: 5792... Loss: 3.9205... ppl: 34.8894  Val Loss: 3.5522\n",
      "Epoch: 3/10 Step: 5824... Loss: 3.2014... ppl: 35.0088  Val Loss: 3.5556\n",
      "Epoch: 3/10 Step: 5856... Loss: 3.4424... ppl: 34.7872  Val Loss: 3.5492\n",
      "Epoch: 3/10 Step: 5888... Loss: 3.1235... ppl: 34.8837  Val Loss: 3.5520\n",
      "Epoch: 3/10 Step: 5920... Loss: 3.5570... ppl: 34.7281  Val Loss: 3.5475\n",
      "Epoch: 3/10 Step: 5952... Loss: 3.9133... ppl: 34.7574  Val Loss: 3.5484\n",
      "Epoch: 3/10 Step: 5984... Loss: 3.3310... ppl: 34.7841  Val Loss: 3.5492\n",
      "Epoch: 3/10 Step: 6016... Loss: 3.7213... ppl: 34.7464  Val Loss: 3.5481\n",
      "Epoch: 3/10 Step: 6048... Loss: 3.4280... ppl: 35.0369  Val Loss: 3.5564\n",
      "Epoch: 3/10 Step: 6080... Loss: 2.7754... ppl: 34.6279  Val Loss: 3.5447\n",
      "Epoch: 3/10 Step: 6112... Loss: 3.3110... ppl: 34.5938  Val Loss: 3.5437\n",
      "Epoch: 3/10 Step: 6144... Loss: 3.0640... ppl: 34.5025  Val Loss: 3.5410\n",
      "Epoch: 3/10 Step: 6176... Loss: 3.2498... ppl: 34.5937  Val Loss: 3.5437\n",
      "Epoch: 3/10 Step: 6208... Loss: 3.4626... ppl: 34.5491  Val Loss: 3.5424\n",
      "Epoch: 3/10 Step: 6240... Loss: 2.7016... ppl: 34.5329  Val Loss: 3.5419\n",
      "Epoch: 3/10 Step: 6272... Loss: 2.7645... ppl: 34.5556  Val Loss: 3.5426\n",
      "Epoch: 3/10 Step: 6304... Loss: 3.1513... ppl: 34.3494  Val Loss: 3.5366\n",
      "Epoch: 3/10 Step: 6336... Loss: 3.4132... ppl: 34.3143  Val Loss: 3.5356\n",
      "Epoch: 3/10 Step: 6368... Loss: 3.0875... ppl: 34.3526  Val Loss: 3.5367\n",
      "Epoch: 3/10 Step: 6400... Loss: 3.6252... ppl: 34.3793  Val Loss: 3.5375\n",
      "Epoch: 3/10 Step: 6432... Loss: 3.6329... ppl: 34.3627  Val Loss: 3.5370\n",
      "Epoch: 3/10 Step: 6464... Loss: 3.6282... ppl: 34.4548  Val Loss: 3.5396\n",
      "Epoch: 3/10 Step: 6496... Loss: 2.8349... ppl: 34.2407  Val Loss: 3.5334\n",
      "Epoch: 3/10 Step: 6528... Loss: 3.1264... ppl: 34.2740  Val Loss: 3.5344\n",
      "Epoch: 3/10 Step: 6560... Loss: 3.2744... ppl: 34.0720  Val Loss: 3.5285\n",
      "Epoch: 3/10 Step: 6592... Loss: 2.5408... ppl: 34.1563  Val Loss: 3.5309\n",
      "Epoch: 3/10 Step: 6624... Loss: 3.6695... ppl: 33.9463  Val Loss: 3.5248\n",
      "Epoch: 3/10 Step: 6656... Loss: 3.0024... ppl: 33.8701  Val Loss: 3.5225\n",
      "Epoch: 3/10 Step: 6688... Loss: 2.7973... ppl: 34.1309  Val Loss: 3.5302\n",
      "Epoch: 3/10 Step: 6720... Loss: 2.4367... ppl: 33.9805  Val Loss: 3.5258\n",
      "Epoch: 3/10 Step: 6752... Loss: 3.5257... ppl: 33.9617  Val Loss: 3.5252\n",
      "Epoch: 3/10 Step: 6784... Loss: 3.1694... ppl: 34.0891  Val Loss: 3.5290\n",
      "Epoch: 3/10 Step: 6816... Loss: 2.7770... ppl: 34.0213  Val Loss: 3.5270\n",
      "Epoch: 3/10 Step: 6848... Loss: 2.8328... ppl: 33.8525  Val Loss: 3.5220\n",
      "Epoch: 3/10 Step: 6880... Loss: 2.9874... ppl: 34.0249  Val Loss: 3.5271\n",
      "Epoch: 3/10 Step: 6912... Loss: 4.1792... ppl: 34.0080  Val Loss: 3.5266\n",
      "Epoch: 3/10 Step: 6944... Loss: 3.1943... ppl: 34.3544  Val Loss: 3.5367\n",
      "Epoch: 3/10 Step: 6976... Loss: 3.4059... ppl: 34.0562  Val Loss: 3.5280\n",
      "Epoch: 3/10 Step: 7008... Loss: 3.3622... ppl: 33.8585  Val Loss: 3.5222\n",
      "Epoch: 4/10 Step: 7040... Loss: 3.5435... ppl: 33.7537  Val Loss: 3.5191\n",
      "Epoch: 4/10 Step: 7072... Loss: 2.9876... ppl: 34.0793  Val Loss: 3.5287\n",
      "Epoch: 4/10 Step: 7104... Loss: 3.2690... ppl: 33.8276  Val Loss: 3.5213\n",
      "Epoch: 4/10 Step: 7136... Loss: 2.8636... ppl: 33.9700  Val Loss: 3.5255\n",
      "Epoch: 4/10 Step: 7168... Loss: 3.4880... ppl: 33.9873  Val Loss: 3.5260\n",
      "Epoch: 4/10 Step: 7200... Loss: 2.7218... ppl: 33.9838  Val Loss: 3.5259\n",
      "Epoch: 4/10 Step: 7232... Loss: 3.0061... ppl: 33.9756  Val Loss: 3.5256\n",
      "Epoch: 4/10 Step: 7264... Loss: 4.2689... ppl: 33.9489  Val Loss: 3.5249\n",
      "Epoch: 4/10 Step: 7296... Loss: 3.5782... ppl: 34.0421  Val Loss: 3.5276\n",
      "Epoch: 4/10 Step: 7328... Loss: 2.5665... ppl: 34.2416  Val Loss: 3.5334\n",
      "Epoch: 4/10 Step: 7360... Loss: 2.8293... ppl: 34.0286  Val Loss: 3.5272\n",
      "Epoch: 4/10 Step: 7392... Loss: 3.2015... ppl: 33.8885  Val Loss: 3.5231\n",
      "Epoch: 4/10 Step: 7424... Loss: 2.7950... ppl: 33.7373  Val Loss: 3.5186\n",
      "Epoch: 4/10 Step: 7456... Loss: 2.6942... ppl: 33.9910  Val Loss: 3.5261\n",
      "Epoch: 4/10 Step: 7488... Loss: 3.4653... ppl: 34.1274  Val Loss: 3.5301\n",
      "Epoch: 4/10 Step: 7520... Loss: 2.0431... ppl: 34.1355  Val Loss: 3.5303\n",
      "Epoch: 4/10 Step: 7552... Loss: 3.1756... ppl: 33.9139  Val Loss: 3.5238\n",
      "Epoch: 4/10 Step: 7584... Loss: 2.6255... ppl: 33.9200  Val Loss: 3.5240\n",
      "Epoch: 4/10 Step: 7616... Loss: 3.0683... ppl: 33.9294  Val Loss: 3.5243\n",
      "Epoch: 4/10 Step: 7648... Loss: 3.4378... ppl: 33.8647  Val Loss: 3.5224\n",
      "Epoch: 4/10 Step: 7680... Loss: 2.8730... ppl: 33.8667  Val Loss: 3.5224\n",
      "Epoch: 4/10 Step: 7712... Loss: 2.8158... ppl: 33.8560  Val Loss: 3.5221\n",
      "Epoch: 4/10 Step: 7744... Loss: 3.2781... ppl: 33.9156  Val Loss: 3.5239\n",
      "Epoch: 4/10 Step: 7776... Loss: 3.6665... ppl: 33.9770  Val Loss: 3.5257\n",
      "Epoch: 4/10 Step: 7808... Loss: 3.5475... ppl: 33.7088  Val Loss: 3.5178\n",
      "Epoch: 4/10 Step: 7840... Loss: 2.1406... ppl: 33.8389  Val Loss: 3.5216\n",
      "Epoch: 4/10 Step: 7872... Loss: 3.6171... ppl: 33.9913  Val Loss: 3.5261\n",
      "Epoch: 4/10 Step: 7904... Loss: 2.3381... ppl: 33.9333  Val Loss: 3.5244\n",
      "Epoch: 4/10 Step: 7936... Loss: 3.1151... ppl: 33.9232  Val Loss: 3.5241\n",
      "Epoch: 4/10 Step: 7968... Loss: 3.7614... ppl: 33.7844  Val Loss: 3.5200\n",
      "Epoch: 4/10 Step: 8000... Loss: 2.9560... ppl: 33.9017  Val Loss: 3.5235\n",
      "Epoch: 4/10 Step: 8032... Loss: 3.3154... ppl: 33.8045  Val Loss: 3.5206\n",
      "Epoch: 4/10 Step: 8064... Loss: 2.5623... ppl: 34.0098  Val Loss: 3.5266\n",
      "Epoch: 4/10 Step: 8096... Loss: 3.0711... ppl: 33.8078  Val Loss: 3.5207\n",
      "Epoch: 4/10 Step: 8128... Loss: 2.2726... ppl: 33.7437  Val Loss: 3.5188\n",
      "Epoch: 4/10 Step: 8160... Loss: 2.9412... ppl: 33.7340  Val Loss: 3.5185\n",
      "Epoch: 4/10 Step: 8192... Loss: 3.2536... ppl: 33.7035  Val Loss: 3.5176\n",
      "Epoch: 4/10 Step: 8224... Loss: 2.9754... ppl: 33.6192  Val Loss: 3.5151\n",
      "Epoch: 4/10 Step: 8256... Loss: 2.6336... ppl: 33.6116  Val Loss: 3.5149\n",
      "Epoch: 4/10 Step: 8288... Loss: 3.1329... ppl: 33.7271  Val Loss: 3.5183\n",
      "Epoch: 4/10 Step: 8320... Loss: 3.1462... ppl: 33.6957  Val Loss: 3.5174\n",
      "Epoch: 4/10 Step: 8352... Loss: 2.8744... ppl: 33.6668  Val Loss: 3.5165\n",
      "Epoch: 4/10 Step: 8384... Loss: 2.6294... ppl: 33.9589  Val Loss: 3.5252\n",
      "Epoch: 4/10 Step: 8416... Loss: 2.7917... ppl: 33.6800  Val Loss: 3.5169\n",
      "Epoch: 4/10 Step: 8448... Loss: 4.1171... ppl: 33.6369  Val Loss: 3.5156\n",
      "Epoch: 4/10 Step: 8480... Loss: 2.7978... ppl: 33.5968  Val Loss: 3.5144\n",
      "Epoch: 4/10 Step: 8512... Loss: 2.4211... ppl: 33.5719  Val Loss: 3.5137\n",
      "Epoch: 4/10 Step: 8544... Loss: 3.0542... ppl: 33.6709  Val Loss: 3.5166\n",
      "Epoch: 4/10 Step: 8576... Loss: 2.9508... ppl: 33.6718  Val Loss: 3.5167\n",
      "Epoch: 4/10 Step: 8608... Loss: 3.4980... ppl: 33.6131  Val Loss: 3.5149\n",
      "Epoch: 4/10 Step: 8640... Loss: 3.2742... ppl: 33.5400  Val Loss: 3.5127\n",
      "Epoch: 4/10 Step: 8672... Loss: 2.6650... ppl: 33.4062  Val Loss: 3.5087\n",
      "Epoch: 4/10 Step: 8704... Loss: 3.6951... ppl: 33.3697  Val Loss: 3.5076\n",
      "Epoch: 4/10 Step: 8736... Loss: 2.9764... ppl: 33.4365  Val Loss: 3.5096\n",
      "Epoch: 4/10 Step: 8768... Loss: 2.6888... ppl: 33.4880  Val Loss: 3.5112\n",
      "Epoch: 4/10 Step: 8800... Loss: 3.5179... ppl: 33.6618  Val Loss: 3.5164\n",
      "Epoch: 4/10 Step: 8832... Loss: 3.0058... ppl: 33.5559  Val Loss: 3.5132\n",
      "Epoch: 4/10 Step: 8864... Loss: 3.2334... ppl: 33.5816  Val Loss: 3.5140\n",
      "Epoch: 4/10 Step: 8896... Loss: 2.6419... ppl: 33.4998  Val Loss: 3.5115\n",
      "Epoch: 4/10 Step: 8928... Loss: 2.7656... ppl: 33.4298  Val Loss: 3.5094\n",
      "Epoch: 4/10 Step: 8960... Loss: 2.3667... ppl: 33.3063  Val Loss: 3.5057\n",
      "Epoch: 4/10 Step: 8992... Loss: 3.0172... ppl: 33.1597  Val Loss: 3.5013\n",
      "Epoch: 4/10 Step: 9024... Loss: 3.4348... ppl: 33.4150  Val Loss: 3.5090\n",
      "Epoch: 4/10 Step: 9056... Loss: 3.1107... ppl: 33.3220  Val Loss: 3.5062\n",
      "Epoch: 4/10 Step: 9088... Loss: 3.3574... ppl: 33.3803  Val Loss: 3.5080\n",
      "Epoch: 4/10 Step: 9120... Loss: 2.6081... ppl: 33.4743  Val Loss: 3.5108\n",
      "Epoch: 4/10 Step: 9152... Loss: 2.9441... ppl: 33.5143  Val Loss: 3.5120\n",
      "Epoch: 4/10 Step: 9184... Loss: 3.3426... ppl: 33.2755  Val Loss: 3.5048\n",
      "Epoch: 4/10 Step: 9216... Loss: 2.4118... ppl: 33.3462  Val Loss: 3.5069\n",
      "Epoch: 4/10 Step: 9248... Loss: 3.6579... ppl: 33.4032  Val Loss: 3.5087\n",
      "Epoch: 4/10 Step: 9280... Loss: 2.7274... ppl: 33.9470  Val Loss: 3.5248\n",
      "Epoch: 4/10 Step: 9312... Loss: 3.3947... ppl: 33.5791  Val Loss: 3.5139\n",
      "Epoch: 4/10 Step: 9344... Loss: 3.2943... ppl: 33.3443  Val Loss: 3.5069\n",
      "Epoch: 5/10 Step: 9376... Loss: 1.9593... ppl: 33.2344  Val Loss: 3.5036\n",
      "Epoch: 5/10 Step: 9408... Loss: 3.2497... ppl: 33.5490  Val Loss: 3.5130\n",
      "Epoch: 5/10 Step: 9440... Loss: 2.8495... ppl: 33.3163  Val Loss: 3.5060\n",
      "Epoch: 5/10 Step: 9472... Loss: 2.7998... ppl: 33.5451  Val Loss: 3.5129\n",
      "Epoch: 5/10 Step: 9504... Loss: 2.9976... ppl: 33.5519  Val Loss: 3.5131\n",
      "Epoch: 5/10 Step: 9536... Loss: 2.1828... ppl: 33.5612  Val Loss: 3.5134\n",
      "Epoch: 5/10 Step: 9568... Loss: 1.9038... ppl: 33.6705  Val Loss: 3.5166\n",
      "Epoch: 5/10 Step: 9600... Loss: 3.0924... ppl: 33.5880  Val Loss: 3.5142\n",
      "Epoch: 5/10 Step: 9632... Loss: 3.0788... ppl: 33.5873  Val Loss: 3.5141\n",
      "Epoch: 5/10 Step: 9664... Loss: 3.2122... ppl: 33.7178  Val Loss: 3.5180\n",
      "Epoch: 5/10 Step: 9696... Loss: 2.2691... ppl: 33.7270  Val Loss: 3.5183\n",
      "Epoch: 5/10 Step: 9728... Loss: 3.0989... ppl: 33.6273  Val Loss: 3.5153\n",
      "Epoch: 5/10 Step: 9760... Loss: 2.3974... ppl: 33.4303  Val Loss: 3.5095\n",
      "Epoch: 5/10 Step: 9792... Loss: 3.4099... ppl: 33.5244  Val Loss: 3.5123\n",
      "Epoch: 5/10 Step: 9824... Loss: 3.1713... ppl: 33.6538  Val Loss: 3.5161\n",
      "Epoch: 5/10 Step: 9856... Loss: 2.1270... ppl: 33.8367  Val Loss: 3.5215\n",
      "Epoch: 5/10 Step: 9888... Loss: 3.2157... ppl: 33.7050  Val Loss: 3.5176\n",
      "Epoch: 5/10 Step: 9920... Loss: 3.0261... ppl: 33.6718  Val Loss: 3.5167\n",
      "Epoch: 5/10 Step: 9952... Loss: 3.2890... ppl: 33.5786  Val Loss: 3.5139\n",
      "Epoch: 5/10 Step: 9984... Loss: 3.1940... ppl: 33.6545  Val Loss: 3.5161\n",
      "Epoch: 5/10 Step: 10016... Loss: 2.7792... ppl: 33.6637  Val Loss: 3.5164\n",
      "Epoch: 5/10 Step: 10048... Loss: 2.7250... ppl: 33.6365  Val Loss: 3.5156\n",
      "Epoch: 5/10 Step: 10080... Loss: 2.9179... ppl: 33.6779  Val Loss: 3.5168\n",
      "Epoch: 5/10 Step: 10112... Loss: 3.8553... ppl: 33.7294  Val Loss: 3.5184\n",
      "Epoch: 5/10 Step: 10144... Loss: 2.7263... ppl: 33.5574  Val Loss: 3.5133\n",
      "Epoch: 5/10 Step: 10176... Loss: 2.4493... ppl: 33.6245  Val Loss: 3.5153\n",
      "Epoch: 5/10 Step: 10208... Loss: 3.2337... ppl: 33.7922  Val Loss: 3.5202\n",
      "Epoch: 5/10 Step: 10240... Loss: 3.0846... ppl: 33.8188  Val Loss: 3.5210\n",
      "Epoch: 5/10 Step: 10272... Loss: 3.6395... ppl: 33.7838  Val Loss: 3.5200\n",
      "Epoch: 5/10 Step: 10304... Loss: 3.6625... ppl: 33.6137  Val Loss: 3.5149\n",
      "Epoch: 5/10 Step: 10336... Loss: 2.9667... ppl: 33.7350  Val Loss: 3.5185\n",
      "Epoch: 5/10 Step: 10368... Loss: 2.8243... ppl: 33.6571  Val Loss: 3.5162\n",
      "Epoch: 5/10 Step: 10400... Loss: 2.9577... ppl: 33.6278  Val Loss: 3.5154\n",
      "Epoch: 5/10 Step: 10432... Loss: 3.0506... ppl: 33.8179  Val Loss: 3.5210\n",
      "Epoch: 5/10 Step: 10464... Loss: 3.1081... ppl: 33.6324  Val Loss: 3.5155\n",
      "Epoch: 5/10 Step: 10496... Loss: 2.6034... ppl: 33.6213  Val Loss: 3.5152\n",
      "Epoch: 5/10 Step: 10528... Loss: 3.1756... ppl: 33.7083  Val Loss: 3.5177\n",
      "Epoch: 5/10 Step: 10560... Loss: 3.5577... ppl: 33.5652  Val Loss: 3.5135\n",
      "Epoch: 5/10 Step: 10592... Loss: 2.9044... ppl: 33.5947  Val Loss: 3.5144\n",
      "Epoch: 5/10 Step: 10624... Loss: 2.9910... ppl: 33.7367  Val Loss: 3.5186\n",
      "Epoch: 5/10 Step: 10656... Loss: 2.9748... ppl: 33.6740  Val Loss: 3.5167\n",
      "Epoch: 5/10 Step: 10688... Loss: 2.7507... ppl: 33.6636  Val Loss: 3.5164\n",
      "Epoch: 5/10 Step: 10720... Loss: 2.9101... ppl: 33.9147  Val Loss: 3.5238\n",
      "Epoch: 5/10 Step: 10752... Loss: 2.6319... ppl: 33.9051  Val Loss: 3.5236\n",
      "Epoch: 5/10 Step: 10784... Loss: 3.4195... ppl: 33.6461  Val Loss: 3.5159\n",
      "Epoch: 5/10 Step: 10816... Loss: 3.0154... ppl: 33.7062  Val Loss: 3.5177\n",
      "Epoch: 5/10 Step: 10848... Loss: 3.4278... ppl: 33.6285  Val Loss: 3.5154\n",
      "Epoch: 5/10 Step: 10880... Loss: 2.8394... ppl: 33.6807  Val Loss: 3.5169\n",
      "Epoch: 5/10 Step: 10912... Loss: 2.3735... ppl: 33.6637  Val Loss: 3.5164\n",
      "Epoch: 5/10 Step: 10944... Loss: 3.1419... ppl: 33.6787  Val Loss: 3.5169\n",
      "Epoch: 5/10 Step: 10976... Loss: 3.0343... ppl: 33.5861  Val Loss: 3.5141\n",
      "Epoch: 5/10 Step: 11008... Loss: 3.2049... ppl: 33.5345  Val Loss: 3.5126\n",
      "Epoch: 5/10 Step: 11040... Loss: 3.5436... ppl: 33.4539  Val Loss: 3.5102\n",
      "Epoch: 5/10 Step: 11072... Loss: 3.5146... ppl: 33.5636  Val Loss: 3.5134\n",
      "Epoch: 5/10 Step: 11104... Loss: 3.1710... ppl: 33.5587  Val Loss: 3.5133\n",
      "Epoch: 5/10 Step: 11136... Loss: 3.7804... ppl: 33.6109  Val Loss: 3.5149\n",
      "Epoch: 5/10 Step: 11168... Loss: 2.3448... ppl: 33.6669  Val Loss: 3.5165\n",
      "Epoch: 5/10 Step: 11200... Loss: 2.9063... ppl: 33.7480  Val Loss: 3.5189\n",
      "Epoch: 5/10 Step: 11232... Loss: 2.7637... ppl: 33.5667  Val Loss: 3.5135\n",
      "Epoch: 5/10 Step: 11264... Loss: 2.1423... ppl: 33.4978  Val Loss: 3.5115\n",
      "Epoch: 5/10 Step: 11296... Loss: 2.7497... ppl: 33.5491  Val Loss: 3.5130\n",
      "Epoch: 5/10 Step: 11328... Loss: 3.6516... ppl: 33.3800  Val Loss: 3.5080\n",
      "Epoch: 5/10 Step: 11360... Loss: 2.8308... ppl: 33.5649  Val Loss: 3.5135\n",
      "Epoch: 5/10 Step: 11392... Loss: 2.6238... ppl: 33.5563  Val Loss: 3.5132\n",
      "Epoch: 5/10 Step: 11424... Loss: 3.5331... ppl: 33.5258  Val Loss: 3.5123\n",
      "Epoch: 5/10 Step: 11456... Loss: 2.2165... ppl: 33.6502  Val Loss: 3.5160\n",
      "Epoch: 5/10 Step: 11488... Loss: 2.8314... ppl: 33.7385  Val Loss: 3.5186\n",
      "Epoch: 5/10 Step: 11520... Loss: 2.3195... ppl: 33.4674  Val Loss: 3.5106\n",
      "Epoch: 5/10 Step: 11552... Loss: 3.8395... ppl: 33.4463  Val Loss: 3.5099\n",
      "Epoch: 5/10 Step: 11584... Loss: 2.8384... ppl: 33.4971  Val Loss: 3.5115\n",
      "Epoch: 5/10 Step: 11616... Loss: 2.3638... ppl: 34.0294  Val Loss: 3.5272\n",
      "Epoch: 5/10 Step: 11648... Loss: 2.7630... ppl: 33.7500  Val Loss: 3.5190\n",
      "Epoch: 5/10 Step: 11680... Loss: 3.2685... ppl: 33.6436  Val Loss: 3.5158\n",
      "Epoch: 5/10 Step: 11712... Loss: 3.0291... ppl: 33.4627  Val Loss: 3.5104\n",
      "Epoch: 6/10 Step: 11744... Loss: 3.6539... ppl: 33.6672  Val Loss: 3.5165\n",
      "Epoch: 6/10 Step: 11776... Loss: 2.9078... ppl: 33.5614  Val Loss: 3.5134\n",
      "Epoch: 6/10 Step: 11808... Loss: 2.5876... ppl: 33.6584  Val Loss: 3.5163\n",
      "Epoch: 6/10 Step: 11840... Loss: 3.3005... ppl: 33.7611  Val Loss: 3.5193\n",
      "Epoch: 6/10 Step: 11872... Loss: 2.9631... ppl: 33.6791  Val Loss: 3.5169\n",
      "Epoch: 6/10 Step: 11904... Loss: 3.0203... ppl: 33.9353  Val Loss: 3.5245\n",
      "Epoch: 6/10 Step: 11936... Loss: 3.0130... ppl: 33.7830  Val Loss: 3.5200\n",
      "Epoch: 6/10 Step: 11968... Loss: 2.7095... ppl: 33.8476  Val Loss: 3.5219\n",
      "Epoch: 6/10 Step: 12000... Loss: 2.5443... ppl: 33.8830  Val Loss: 3.5229\n",
      "Epoch: 6/10 Step: 12032... Loss: 2.9569... ppl: 34.1685  Val Loss: 3.5313\n",
      "Epoch: 6/10 Step: 12064... Loss: 3.0240... ppl: 33.9338  Val Loss: 3.5244\n",
      "Epoch: 6/10 Step: 12096... Loss: 3.7876... ppl: 33.7090  Val Loss: 3.5178\n",
      "Epoch: 6/10 Step: 12128... Loss: 2.5104... ppl: 33.6949  Val Loss: 3.5173\n",
      "Epoch: 6/10 Step: 12160... Loss: 2.9089... ppl: 33.9249  Val Loss: 3.5241\n",
      "Epoch: 6/10 Step: 12192... Loss: 2.7859... ppl: 34.1661  Val Loss: 3.5312\n",
      "Epoch: 6/10 Step: 12224... Loss: 2.6021... ppl: 34.0936  Val Loss: 3.5291\n",
      "Epoch: 6/10 Step: 12256... Loss: 2.6169... ppl: 34.0069  Val Loss: 3.5266\n",
      "Epoch: 6/10 Step: 12288... Loss: 2.8362... ppl: 33.8976  Val Loss: 3.5233\n",
      "Epoch: 6/10 Step: 12320... Loss: 2.8323... ppl: 33.9997  Val Loss: 3.5264\n",
      "Epoch: 6/10 Step: 12352... Loss: 2.6796... ppl: 34.0300  Val Loss: 3.5272\n",
      "Epoch: 6/10 Step: 12384... Loss: 2.9444... ppl: 34.0781  Val Loss: 3.5287\n",
      "Epoch: 6/10 Step: 12416... Loss: 2.9333... ppl: 34.1480  Val Loss: 3.5307\n",
      "Epoch: 6/10 Step: 12448... Loss: 2.9422... ppl: 34.0602  Val Loss: 3.5281\n",
      "Epoch: 6/10 Step: 12480... Loss: 2.5837... ppl: 34.0570  Val Loss: 3.5280\n",
      "Epoch: 6/10 Step: 12512... Loss: 2.8566... ppl: 33.9470  Val Loss: 3.5248\n",
      "Epoch: 6/10 Step: 12544... Loss: 2.8532... ppl: 34.1774  Val Loss: 3.5316\n",
      "Epoch: 6/10 Step: 12576... Loss: 2.4625... ppl: 34.3901  Val Loss: 3.5378\n",
      "Epoch: 6/10 Step: 12608... Loss: 3.1922... ppl: 34.1251  Val Loss: 3.5300\n",
      "Epoch: 6/10 Step: 12640... Loss: 2.5849... ppl: 34.0716  Val Loss: 3.5285\n",
      "Epoch: 6/10 Step: 12672... Loss: 3.0864... ppl: 34.1766  Val Loss: 3.5315\n",
      "Epoch: 6/10 Step: 12704... Loss: 2.4988... ppl: 34.1489  Val Loss: 3.5307\n",
      "Epoch: 6/10 Step: 12736... Loss: 3.3167... ppl: 34.0075  Val Loss: 3.5266\n",
      "Epoch: 6/10 Step: 12768... Loss: 2.5485... ppl: 34.3647  Val Loss: 3.5370\n",
      "Epoch: 6/10 Step: 12800... Loss: 2.9224... ppl: 34.1302  Val Loss: 3.5302\n",
      "Epoch: 6/10 Step: 12832... Loss: 2.9807... ppl: 34.0963  Val Loss: 3.5292\n",
      "Epoch: 6/10 Step: 12864... Loss: 2.9038... ppl: 34.2089  Val Loss: 3.5325\n",
      "Epoch: 6/10 Step: 12896... Loss: 3.2419... ppl: 33.9669  Val Loss: 3.5254\n",
      "Epoch: 6/10 Step: 12928... Loss: 2.7015... ppl: 34.0723  Val Loss: 3.5285\n",
      "Epoch: 6/10 Step: 12960... Loss: 2.8819... ppl: 34.1874  Val Loss: 3.5319\n",
      "Epoch: 6/10 Step: 12992... Loss: 2.9671... ppl: 34.1258  Val Loss: 3.5301\n",
      "Epoch: 6/10 Step: 13024... Loss: 2.3653... ppl: 34.2386  Val Loss: 3.5334\n",
      "Epoch: 6/10 Step: 13056... Loss: 2.7300... ppl: 34.3354  Val Loss: 3.5362\n",
      "Epoch: 6/10 Step: 13088... Loss: 2.6137... ppl: 34.5250  Val Loss: 3.5417\n",
      "Epoch: 6/10 Step: 13120... Loss: 3.2927... ppl: 34.1825  Val Loss: 3.5317\n",
      "Epoch: 6/10 Step: 13152... Loss: 2.6841... ppl: 34.3449  Val Loss: 3.5365\n",
      "Epoch: 6/10 Step: 13184... Loss: 3.2929... ppl: 34.2016  Val Loss: 3.5323\n",
      "Epoch: 6/10 Step: 13216... Loss: 3.4860... ppl: 34.1599  Val Loss: 3.5311\n",
      "Epoch: 6/10 Step: 13248... Loss: 3.4322... ppl: 34.1328  Val Loss: 3.5303\n",
      "Epoch: 6/10 Step: 13280... Loss: 3.3644... ppl: 34.2811  Val Loss: 3.5346\n",
      "Epoch: 6/10 Step: 13312... Loss: 2.9240... ppl: 34.1985  Val Loss: 3.5322\n",
      "Epoch: 6/10 Step: 13344... Loss: 2.5357... ppl: 34.2255  Val Loss: 3.5330\n",
      "Epoch: 6/10 Step: 13376... Loss: 2.6179... ppl: 34.0238  Val Loss: 3.5271\n",
      "Epoch: 6/10 Step: 13408... Loss: 2.1977... ppl: 34.1330  Val Loss: 3.5303\n",
      "Epoch: 6/10 Step: 13440... Loss: 3.3216... ppl: 34.1970  Val Loss: 3.5321\n",
      "Epoch: 6/10 Step: 13472... Loss: 3.2735... ppl: 34.2343  Val Loss: 3.5332\n",
      "Epoch: 6/10 Step: 13504... Loss: 2.9068... ppl: 34.3969  Val Loss: 3.5380\n",
      "Epoch: 6/10 Step: 13536... Loss: 3.1417... ppl: 34.4242  Val Loss: 3.5388\n",
      "Epoch: 6/10 Step: 13568... Loss: 2.7550... ppl: 34.2203  Val Loss: 3.5328\n",
      "Epoch: 6/10 Step: 13600... Loss: 2.7800... ppl: 34.1401  Val Loss: 3.5305\n",
      "Epoch: 6/10 Step: 13632... Loss: 3.6994... ppl: 34.3184  Val Loss: 3.5357\n",
      "Epoch: 6/10 Step: 13664... Loss: 2.8389... ppl: 33.9696  Val Loss: 3.5255\n",
      "Epoch: 6/10 Step: 13696... Loss: 3.1571... ppl: 34.0076  Val Loss: 3.5266\n",
      "Epoch: 6/10 Step: 13728... Loss: 2.4886... ppl: 34.1852  Val Loss: 3.5318\n",
      "Epoch: 6/10 Step: 13760... Loss: 3.2810... ppl: 34.0236  Val Loss: 3.5271\n",
      "Epoch: 6/10 Step: 13792... Loss: 3.0540... ppl: 34.1701  Val Loss: 3.5314\n",
      "Epoch: 6/10 Step: 13824... Loss: 2.4643... ppl: 34.3237  Val Loss: 3.5358\n",
      "Epoch: 6/10 Step: 13856... Loss: 3.2316... ppl: 34.2470  Val Loss: 3.5336\n",
      "Epoch: 6/10 Step: 13888... Loss: 2.8820... ppl: 34.0632  Val Loss: 3.5282\n",
      "Epoch: 6/10 Step: 13920... Loss: 3.2987... ppl: 34.0510  Val Loss: 3.5279\n",
      "Epoch: 6/10 Step: 13952... Loss: 2.1031... ppl: 34.4928  Val Loss: 3.5408\n",
      "Epoch: 6/10 Step: 13984... Loss: 3.0365... ppl: 34.6534  Val Loss: 3.5454\n",
      "Epoch: 6/10 Step: 14016... Loss: 2.5612... ppl: 34.3822  Val Loss: 3.5375\n",
      "Epoch: 6/10 Step: 14048... Loss: 2.5393... ppl: 34.0774  Val Loss: 3.5286\n",
      "Epoch: 7/10 Step: 14080... Loss: 3.0042... ppl: 34.2176  Val Loss: 3.5327\n",
      "Epoch: 7/10 Step: 14112... Loss: 2.1299... ppl: 34.3946  Val Loss: 3.5379\n",
      "Epoch: 7/10 Step: 14144... Loss: 2.6877... ppl: 34.2231  Val Loss: 3.5329\n",
      "Epoch: 7/10 Step: 14176... Loss: 2.7040... ppl: 34.3216  Val Loss: 3.5358\n",
      "Epoch: 7/10 Step: 14208... Loss: 2.4619... ppl: 34.2798  Val Loss: 3.5346\n",
      "Epoch: 7/10 Step: 14240... Loss: 2.7245... ppl: 34.4073  Val Loss: 3.5383\n",
      "Epoch: 7/10 Step: 14272... Loss: 2.4403... ppl: 34.5406  Val Loss: 3.5421\n",
      "Epoch: 7/10 Step: 14304... Loss: 2.3124... ppl: 34.5294  Val Loss: 3.5418\n",
      "Epoch: 7/10 Step: 14336... Loss: 2.7160... ppl: 34.5734  Val Loss: 3.5431\n",
      "Epoch: 7/10 Step: 14368... Loss: 3.5579... ppl: 34.8982  Val Loss: 3.5524\n",
      "Epoch: 7/10 Step: 14400... Loss: 2.6306... ppl: 34.5908  Val Loss: 3.5436\n",
      "Epoch: 7/10 Step: 14432... Loss: 3.0027... ppl: 34.4797  Val Loss: 3.5404\n",
      "Epoch: 7/10 Step: 14464... Loss: 3.0638... ppl: 34.2432  Val Loss: 3.5335\n",
      "Epoch: 7/10 Step: 14496... Loss: 3.0360... ppl: 34.6568  Val Loss: 3.5455\n",
      "Epoch: 7/10 Step: 14528... Loss: 2.1139... ppl: 34.8064  Val Loss: 3.5498\n",
      "Epoch: 7/10 Step: 14560... Loss: 2.3847... ppl: 34.8028  Val Loss: 3.5497\n",
      "Epoch: 7/10 Step: 14592... Loss: 2.4797... ppl: 34.6521  Val Loss: 3.5454\n",
      "Epoch: 7/10 Step: 14624... Loss: 2.1445... ppl: 34.6053  Val Loss: 3.5440\n",
      "Epoch: 7/10 Step: 14656... Loss: 3.0614... ppl: 34.7795  Val Loss: 3.5490\n",
      "Epoch: 7/10 Step: 14688... Loss: 3.0008... ppl: 34.6667  Val Loss: 3.5458\n",
      "Epoch: 7/10 Step: 14720... Loss: 3.7711... ppl: 34.7572  Val Loss: 3.5484\n",
      "Epoch: 7/10 Step: 14752... Loss: 3.0122... ppl: 34.8907  Val Loss: 3.5522\n",
      "Epoch: 7/10 Step: 14784... Loss: 2.1636... ppl: 34.7974  Val Loss: 3.5495\n",
      "Epoch: 7/10 Step: 14816... Loss: 2.6915... ppl: 34.9073  Val Loss: 3.5527\n",
      "Epoch: 7/10 Step: 14848... Loss: 2.6050... ppl: 34.6305  Val Loss: 3.5447\n",
      "Epoch: 7/10 Step: 14880... Loss: 2.8041... ppl: 34.9208  Val Loss: 3.5531\n",
      "Epoch: 7/10 Step: 14912... Loss: 2.4952... ppl: 35.1778  Val Loss: 3.5604\n",
      "Epoch: 7/10 Step: 14944... Loss: 2.9875... ppl: 34.9483  Val Loss: 3.5539\n",
      "Epoch: 7/10 Step: 14976... Loss: 2.6655... ppl: 34.7871  Val Loss: 3.5492\n",
      "Epoch: 7/10 Step: 15008... Loss: 2.9205... ppl: 34.8970  Val Loss: 3.5524\n",
      "Epoch: 7/10 Step: 15040... Loss: 2.1214... ppl: 34.9210  Val Loss: 3.5531\n",
      "Epoch: 7/10 Step: 15072... Loss: 3.0903... ppl: 34.7411  Val Loss: 3.5479\n",
      "Epoch: 7/10 Step: 15104... Loss: 2.7614... ppl: 35.0730  Val Loss: 3.5574\n",
      "Epoch: 7/10 Step: 15136... Loss: 3.1936... ppl: 34.9839  Val Loss: 3.5549\n",
      "Epoch: 7/10 Step: 15168... Loss: 2.3360... ppl: 34.9021  Val Loss: 3.5525\n",
      "Epoch: 7/10 Step: 15200... Loss: 2.6637... ppl: 34.9275  Val Loss: 3.5533\n",
      "Epoch: 7/10 Step: 15232... Loss: 3.1438... ppl: 34.6802  Val Loss: 3.5462\n",
      "Epoch: 7/10 Step: 15264... Loss: 2.8338... ppl: 34.7829  Val Loss: 3.5491\n",
      "Epoch: 7/10 Step: 15296... Loss: 2.8878... ppl: 34.8506  Val Loss: 3.5511\n",
      "Epoch: 7/10 Step: 15328... Loss: 2.6097... ppl: 34.8479  Val Loss: 3.5510\n",
      "Epoch: 7/10 Step: 15360... Loss: 2.8301... ppl: 35.0056  Val Loss: 3.5555\n",
      "Epoch: 7/10 Step: 15392... Loss: 2.7532... ppl: 35.0027  Val Loss: 3.5554\n",
      "Epoch: 7/10 Step: 15424... Loss: 2.2538... ppl: 35.3174  Val Loss: 3.5644\n",
      "Epoch: 7/10 Step: 15456... Loss: 3.0785... ppl: 35.0258  Val Loss: 3.5561\n",
      "Epoch: 7/10 Step: 15488... Loss: 2.7369... ppl: 35.0343  Val Loss: 3.5563\n",
      "Epoch: 7/10 Step: 15520... Loss: 3.0162... ppl: 34.9632  Val Loss: 3.5543\n",
      "Epoch: 7/10 Step: 15552... Loss: 2.5535... ppl: 34.8806  Val Loss: 3.5519\n",
      "Epoch: 7/10 Step: 15584... Loss: 3.2439... ppl: 34.8937  Val Loss: 3.5523\n",
      "Epoch: 7/10 Step: 15616... Loss: 2.1200... ppl: 35.0264  Val Loss: 3.5561\n",
      "Epoch: 7/10 Step: 15648... Loss: 2.8844... ppl: 35.0613  Val Loss: 3.5571\n",
      "Epoch: 7/10 Step: 15680... Loss: 2.7106... ppl: 34.9868  Val Loss: 3.5550\n",
      "Epoch: 7/10 Step: 15712... Loss: 2.2220... ppl: 34.7823  Val Loss: 3.5491\n",
      "Epoch: 7/10 Step: 15744... Loss: 2.8147... ppl: 34.8670  Val Loss: 3.5515\n",
      "Epoch: 7/10 Step: 15776... Loss: 2.9296... ppl: 34.9242  Val Loss: 3.5532\n",
      "Epoch: 7/10 Step: 15808... Loss: 3.1333... ppl: 35.0220  Val Loss: 3.5560\n",
      "Epoch: 7/10 Step: 15840... Loss: 3.4408... ppl: 35.1207  Val Loss: 3.5588\n",
      "Epoch: 7/10 Step: 15872... Loss: 3.3118... ppl: 35.0859  Val Loss: 3.5578\n",
      "Epoch: 7/10 Step: 15904... Loss: 2.4483... ppl: 34.9676  Val Loss: 3.5544\n",
      "Epoch: 7/10 Step: 15936... Loss: 2.4823... ppl: 34.9799  Val Loss: 3.5548\n",
      "Epoch: 7/10 Step: 15968... Loss: 1.9461... ppl: 35.0238  Val Loss: 3.5560\n",
      "Epoch: 7/10 Step: 16000... Loss: 2.8041... ppl: 34.7437  Val Loss: 3.5480\n",
      "Epoch: 7/10 Step: 16032... Loss: 2.5647... ppl: 34.6711  Val Loss: 3.5459\n",
      "Epoch: 7/10 Step: 16064... Loss: 2.3477... ppl: 34.9339  Val Loss: 3.5535\n",
      "Epoch: 7/10 Step: 16096... Loss: 2.2524... ppl: 34.7894  Val Loss: 3.5493\n",
      "Epoch: 7/10 Step: 16128... Loss: 2.4804... ppl: 34.9217  Val Loss: 3.5531\n",
      "Epoch: 7/10 Step: 16160... Loss: 2.7630... ppl: 35.1232  Val Loss: 3.5589\n",
      "Epoch: 7/10 Step: 16192... Loss: 2.6212... ppl: 35.1132  Val Loss: 3.5586\n",
      "Epoch: 7/10 Step: 16224... Loss: 3.1151... ppl: 34.7926  Val Loss: 3.5494\n",
      "Epoch: 7/10 Step: 16256... Loss: 2.4503... ppl: 34.8866  Val Loss: 3.5521\n",
      "Epoch: 7/10 Step: 16288... Loss: 2.7208... ppl: 35.0464  Val Loss: 3.5567\n",
      "Epoch: 7/10 Step: 16320... Loss: 2.3524... ppl: 35.6246  Val Loss: 3.5730\n",
      "Epoch: 7/10 Step: 16352... Loss: 2.4873... ppl: 35.3595  Val Loss: 3.5656\n",
      "Epoch: 7/10 Step: 16384... Loss: 3.5072... ppl: 34.9929  Val Loss: 3.5551\n",
      "Epoch: 8/10 Step: 16416... Loss: 3.1752... ppl: 34.8412  Val Loss: 3.5508\n",
      "Epoch: 8/10 Step: 16448... Loss: 2.6550... ppl: 35.2651  Val Loss: 3.5629\n",
      "Epoch: 8/10 Step: 16480... Loss: 2.6606... ppl: 34.9962  Val Loss: 3.5552\n",
      "Epoch: 8/10 Step: 16512... Loss: 1.7925... ppl: 35.0526  Val Loss: 3.5568\n",
      "Epoch: 8/10 Step: 16544... Loss: 2.6409... ppl: 35.2086  Val Loss: 3.5613\n",
      "Epoch: 8/10 Step: 16576... Loss: 2.8681... ppl: 35.1923  Val Loss: 3.5608\n",
      "Epoch: 8/10 Step: 16608... Loss: 2.8499... ppl: 35.3911  Val Loss: 3.5665\n",
      "Epoch: 8/10 Step: 16640... Loss: 2.4884... ppl: 35.3288  Val Loss: 3.5647\n",
      "Epoch: 8/10 Step: 16672... Loss: 2.7984... ppl: 35.3506  Val Loss: 3.5653\n",
      "Epoch: 8/10 Step: 16704... Loss: 2.7494... ppl: 35.7805  Val Loss: 3.5774\n",
      "Epoch: 8/10 Step: 16736... Loss: 3.0037... ppl: 35.6658  Val Loss: 3.5742\n",
      "Epoch: 8/10 Step: 16768... Loss: 2.8569... ppl: 35.4182  Val Loss: 3.5672\n",
      "Epoch: 8/10 Step: 16800... Loss: 2.2023... ppl: 35.1154  Val Loss: 3.5586\n",
      "Epoch: 8/10 Step: 16832... Loss: 2.4661... ppl: 35.2828  Val Loss: 3.5634\n",
      "Epoch: 8/10 Step: 16864... Loss: 2.6413... ppl: 35.5716  Val Loss: 3.5715\n",
      "Epoch: 8/10 Step: 16896... Loss: 2.3221... ppl: 35.7609  Val Loss: 3.5769\n",
      "Epoch: 8/10 Step: 16928... Loss: 2.4917... ppl: 35.4667  Val Loss: 3.5686\n",
      "Epoch: 8/10 Step: 16960... Loss: 2.4587... ppl: 35.5425  Val Loss: 3.5707\n",
      "Epoch: 8/10 Step: 16992... Loss: 2.6498... ppl: 35.5857  Val Loss: 3.5719\n",
      "Epoch: 8/10 Step: 17024... Loss: 2.1859... ppl: 35.3918  Val Loss: 3.5665\n",
      "Epoch: 8/10 Step: 17056... Loss: 2.5204... ppl: 35.5982  Val Loss: 3.5723\n",
      "Epoch: 8/10 Step: 17088... Loss: 3.3499... ppl: 35.7292  Val Loss: 3.5760\n",
      "Epoch: 8/10 Step: 17120... Loss: 2.6519... ppl: 35.7330  Val Loss: 3.5761\n",
      "Epoch: 8/10 Step: 17152... Loss: 3.1373... ppl: 35.7866  Val Loss: 3.5776\n",
      "Epoch: 8/10 Step: 17184... Loss: 2.5527... ppl: 35.5414  Val Loss: 3.5707\n",
      "Epoch: 8/10 Step: 17216... Loss: 2.4165... ppl: 35.7408  Val Loss: 3.5763\n",
      "Epoch: 8/10 Step: 17248... Loss: 2.2461... ppl: 36.0349  Val Loss: 3.5845\n",
      "Epoch: 8/10 Step: 17280... Loss: 2.3478... ppl: 35.8504  Val Loss: 3.5794\n",
      "Epoch: 8/10 Step: 17312... Loss: 3.4109... ppl: 35.7413  Val Loss: 3.5763\n",
      "Epoch: 8/10 Step: 17344... Loss: 2.6017... ppl: 35.8250  Val Loss: 3.5786\n",
      "Epoch: 8/10 Step: 17376... Loss: 2.7725... ppl: 35.8955  Val Loss: 3.5806\n",
      "Epoch: 8/10 Step: 17408... Loss: 2.7701... ppl: 35.7121  Val Loss: 3.5755\n",
      "Epoch: 8/10 Step: 17440... Loss: 2.6980... ppl: 35.9228  Val Loss: 3.5814\n",
      "Epoch: 8/10 Step: 17472... Loss: 2.7030... ppl: 35.9702  Val Loss: 3.5827\n",
      "Epoch: 8/10 Step: 17504... Loss: 2.7142... ppl: 35.8384  Val Loss: 3.5790\n",
      "Epoch: 8/10 Step: 17536... Loss: 2.7528... ppl: 35.7990  Val Loss: 3.5779\n",
      "Epoch: 8/10 Step: 17568... Loss: 2.8374... ppl: 35.6262  Val Loss: 3.5731\n",
      "Epoch: 8/10 Step: 17600... Loss: 2.6307... ppl: 35.5567  Val Loss: 3.5711\n",
      "Epoch: 8/10 Step: 17632... Loss: 2.6515... ppl: 35.7277  Val Loss: 3.5759\n",
      "Epoch: 8/10 Step: 17664... Loss: 2.5067... ppl: 35.6903  Val Loss: 3.5749\n",
      "Epoch: 8/10 Step: 17696... Loss: 3.1168... ppl: 35.7909  Val Loss: 3.5777\n",
      "Epoch: 8/10 Step: 17728... Loss: 3.0693... ppl: 35.7457  Val Loss: 3.5764\n",
      "Epoch: 8/10 Step: 17760... Loss: 2.7245... ppl: 36.1241  Val Loss: 3.5870\n",
      "Epoch: 8/10 Step: 17792... Loss: 2.5051... ppl: 35.9918  Val Loss: 3.5833\n",
      "Epoch: 8/10 Step: 17824... Loss: 2.7939... ppl: 35.9708  Val Loss: 3.5827\n",
      "Epoch: 8/10 Step: 17856... Loss: 2.3253... ppl: 35.8563  Val Loss: 3.5795\n",
      "Epoch: 8/10 Step: 17888... Loss: 2.7702... ppl: 35.7725  Val Loss: 3.5772\n",
      "Epoch: 8/10 Step: 17920... Loss: 2.2810... ppl: 35.8003  Val Loss: 3.5780\n",
      "Epoch: 8/10 Step: 17952... Loss: 2.7622... ppl: 35.7937  Val Loss: 3.5778\n",
      "Epoch: 8/10 Step: 17984... Loss: 2.7295... ppl: 35.9689  Val Loss: 3.5827\n",
      "Epoch: 8/10 Step: 18016... Loss: 2.7838... ppl: 35.9897  Val Loss: 3.5832\n",
      "Epoch: 8/10 Step: 18048... Loss: 3.1159... ppl: 35.7008  Val Loss: 3.5752\n",
      "Epoch: 8/10 Step: 18080... Loss: 2.9809... ppl: 35.7850  Val Loss: 3.5775\n",
      "Epoch: 8/10 Step: 18112... Loss: 2.3197... ppl: 35.9001  Val Loss: 3.5807\n",
      "Epoch: 8/10 Step: 18144... Loss: 3.4018... ppl: 35.9510  Val Loss: 3.5822\n",
      "Epoch: 8/10 Step: 18176... Loss: 2.9041... ppl: 36.0182  Val Loss: 3.5840\n",
      "Epoch: 8/10 Step: 18208... Loss: 2.7754... ppl: 36.0235  Val Loss: 3.5842\n",
      "Epoch: 8/10 Step: 18240... Loss: 2.8116... ppl: 36.1259  Val Loss: 3.5870\n",
      "Epoch: 8/10 Step: 18272... Loss: 1.9867... ppl: 35.9866  Val Loss: 3.5831\n",
      "Epoch: 8/10 Step: 18304... Loss: 2.9893... ppl: 36.0015  Val Loss: 3.5836\n",
      "Epoch: 8/10 Step: 18336... Loss: 3.1199... ppl: 35.8645  Val Loss: 3.5797\n",
      "Epoch: 8/10 Step: 18368... Loss: 3.0221... ppl: 35.5559  Val Loss: 3.5711\n",
      "Epoch: 8/10 Step: 18400... Loss: 2.7637... ppl: 35.9999  Val Loss: 3.5835\n",
      "Epoch: 8/10 Step: 18432... Loss: 3.1007... ppl: 35.9068  Val Loss: 3.5809\n",
      "Epoch: 8/10 Step: 18464... Loss: 2.4236... ppl: 35.8418  Val Loss: 3.5791\n",
      "Epoch: 8/10 Step: 18496... Loss: 2.1684... ppl: 36.0764  Val Loss: 3.5856\n",
      "Epoch: 8/10 Step: 18528... Loss: 2.7559... ppl: 36.0066  Val Loss: 3.5837\n",
      "Epoch: 8/10 Step: 18560... Loss: 2.4591... ppl: 35.7875  Val Loss: 3.5776\n",
      "Epoch: 8/10 Step: 18592... Loss: 2.7790... ppl: 35.8151  Val Loss: 3.5784\n",
      "Epoch: 8/10 Step: 18624... Loss: 2.6968... ppl: 35.8783  Val Loss: 3.5801\n",
      "Epoch: 8/10 Step: 18656... Loss: 3.5204... ppl: 36.7534  Val Loss: 3.6042\n",
      "Epoch: 8/10 Step: 18688... Loss: 2.2215... ppl: 36.4341  Val Loss: 3.5955\n",
      "Epoch: 8/10 Step: 18720... Loss: 2.4860... ppl: 36.0900  Val Loss: 3.5860\n",
      "Epoch: 9/10 Step: 18752... Loss: 2.3761... ppl: 35.8591  Val Loss: 3.5796\n",
      "Epoch: 9/10 Step: 18784... Loss: 2.5397... ppl: 36.2459  Val Loss: 3.5903\n",
      "Epoch: 9/10 Step: 18816... Loss: 2.8443... ppl: 36.0241  Val Loss: 3.5842\n",
      "Epoch: 9/10 Step: 18848... Loss: 2.9527... ppl: 35.9373  Val Loss: 3.5818\n",
      "Epoch: 9/10 Step: 18880... Loss: 2.8369... ppl: 36.2532  Val Loss: 3.5905\n",
      "Epoch: 9/10 Step: 18912... Loss: 2.8616... ppl: 36.1762  Val Loss: 3.5884\n",
      "Epoch: 9/10 Step: 18944... Loss: 2.7309... ppl: 36.3940  Val Loss: 3.5944\n",
      "Epoch: 9/10 Step: 18976... Loss: 2.7143... ppl: 36.2877  Val Loss: 3.5915\n",
      "Epoch: 9/10 Step: 19008... Loss: 3.0692... ppl: 36.3384  Val Loss: 3.5929\n",
      "Epoch: 9/10 Step: 19040... Loss: 2.3914... ppl: 36.6073  Val Loss: 3.6002\n",
      "Epoch: 9/10 Step: 19072... Loss: 2.5989... ppl: 36.6156  Val Loss: 3.6005\n",
      "Epoch: 9/10 Step: 19104... Loss: 2.6618... ppl: 36.4025  Val Loss: 3.5946\n",
      "Epoch: 9/10 Step: 19136... Loss: 1.9620... ppl: 36.0735  Val Loss: 3.5856\n",
      "Epoch: 9/10 Step: 19168... Loss: 2.3160... ppl: 36.0874  Val Loss: 3.5859\n",
      "Epoch: 9/10 Step: 19200... Loss: 2.9935... ppl: 36.4339  Val Loss: 3.5955\n",
      "Epoch: 9/10 Step: 19232... Loss: 2.9957... ppl: 36.8182  Val Loss: 3.6060\n",
      "Epoch: 9/10 Step: 19264... Loss: 2.8688... ppl: 36.6143  Val Loss: 3.6004\n",
      "Epoch: 9/10 Step: 19296... Loss: 3.1414... ppl: 36.6510  Val Loss: 3.6014\n",
      "Epoch: 9/10 Step: 19328... Loss: 2.8451... ppl: 36.6258  Val Loss: 3.6008\n",
      "Epoch: 9/10 Step: 19360... Loss: 3.0096... ppl: 36.5993  Val Loss: 3.6000\n",
      "Epoch: 9/10 Step: 19392... Loss: 2.3667... ppl: 36.6763  Val Loss: 3.6021\n",
      "Epoch: 9/10 Step: 19424... Loss: 2.7525... ppl: 36.7257  Val Loss: 3.6035\n",
      "Epoch: 9/10 Step: 19456... Loss: 2.6126... ppl: 36.7529  Val Loss: 3.6042\n",
      "Epoch: 9/10 Step: 19488... Loss: 2.5384... ppl: 36.8031  Val Loss: 3.6056\n",
      "Epoch: 9/10 Step: 19520... Loss: 2.6935... ppl: 36.7377  Val Loss: 3.6038\n",
      "Epoch: 9/10 Step: 19552... Loss: 2.1791... ppl: 36.7823  Val Loss: 3.6050\n",
      "Epoch: 9/10 Step: 19584... Loss: 2.2321... ppl: 37.0059  Val Loss: 3.6111\n",
      "Epoch: 9/10 Step: 19616... Loss: 2.8316... ppl: 36.8880  Val Loss: 3.6079\n",
      "Epoch: 9/10 Step: 19648... Loss: 2.8722... ppl: 36.7697  Val Loss: 3.6047\n",
      "Epoch: 9/10 Step: 19680... Loss: 2.5568... ppl: 36.7057  Val Loss: 3.6029\n",
      "Epoch: 9/10 Step: 19712... Loss: 2.2327... ppl: 36.8783  Val Loss: 3.6076\n",
      "Epoch: 9/10 Step: 19744... Loss: 2.0104... ppl: 36.9005  Val Loss: 3.6082\n",
      "Epoch: 9/10 Step: 19776... Loss: 2.9699... ppl: 36.8848  Val Loss: 3.6078\n",
      "Epoch: 9/10 Step: 19808... Loss: 2.1478... ppl: 37.1345  Val Loss: 3.6145\n",
      "Epoch: 9/10 Step: 19840... Loss: 2.3900... ppl: 36.9529  Val Loss: 3.6096\n",
      "Epoch: 9/10 Step: 19872... Loss: 2.6924... ppl: 36.9511  Val Loss: 3.6096\n",
      "Epoch: 9/10 Step: 19904... Loss: 2.4697... ppl: 36.8737  Val Loss: 3.6075\n",
      "Epoch: 9/10 Step: 19936... Loss: 2.4870... ppl: 36.5985  Val Loss: 3.6000\n",
      "Epoch: 9/10 Step: 19968... Loss: 2.5773... ppl: 36.6232  Val Loss: 3.6007\n",
      "Epoch: 9/10 Step: 20000... Loss: 3.0814... ppl: 36.6850  Val Loss: 3.6024\n",
      "Epoch: 9/10 Step: 20032... Loss: 2.8586... ppl: 36.7237  Val Loss: 3.6034\n",
      "Epoch: 9/10 Step: 20064... Loss: 2.3787... ppl: 36.7392  Val Loss: 3.6038\n",
      "Epoch: 9/10 Step: 20096... Loss: 2.5949... ppl: 37.0236  Val Loss: 3.6116\n",
      "Epoch: 9/10 Step: 20128... Loss: 2.7292... ppl: 37.1921  Val Loss: 3.6161\n",
      "Epoch: 9/10 Step: 20160... Loss: 2.8505... ppl: 36.9080  Val Loss: 3.6084\n",
      "Epoch: 9/10 Step: 20192... Loss: 2.2927... ppl: 36.9259  Val Loss: 3.6089\n",
      "Epoch: 9/10 Step: 20224... Loss: 2.4349... ppl: 36.8279  Val Loss: 3.6063\n",
      "Epoch: 9/10 Step: 20256... Loss: 2.4479... ppl: 36.8217  Val Loss: 3.6061\n",
      "Epoch: 9/10 Step: 20288... Loss: 2.6698... ppl: 36.7736  Val Loss: 3.6048\n",
      "Epoch: 9/10 Step: 20320... Loss: 2.6430... ppl: 37.0558  Val Loss: 3.6124\n",
      "Epoch: 9/10 Step: 20352... Loss: 2.3895... ppl: 37.0584  Val Loss: 3.6125\n",
      "Epoch: 9/10 Step: 20384... Loss: 2.7734... ppl: 36.8780  Val Loss: 3.6076\n",
      "Epoch: 9/10 Step: 20416... Loss: 2.4468... ppl: 36.8213  Val Loss: 3.6061\n",
      "Epoch: 9/10 Step: 20448... Loss: 2.4694... ppl: 36.8964  Val Loss: 3.6081\n",
      "Epoch: 9/10 Step: 20480... Loss: 2.5218... ppl: 36.8978  Val Loss: 3.6082\n",
      "Epoch: 9/10 Step: 20512... Loss: 2.8996... ppl: 36.9404  Val Loss: 3.6093\n",
      "Epoch: 9/10 Step: 20544... Loss: 2.5857... ppl: 37.1952  Val Loss: 3.6162\n",
      "Epoch: 9/10 Step: 20576... Loss: 2.6667... ppl: 37.2810  Val Loss: 3.6185\n",
      "Epoch: 9/10 Step: 20608... Loss: 2.5309... ppl: 37.1237  Val Loss: 3.6143\n",
      "Epoch: 9/10 Step: 20640... Loss: 2.2824... ppl: 37.0685  Val Loss: 3.6128\n",
      "Epoch: 9/10 Step: 20672... Loss: 2.6445... ppl: 36.9159  Val Loss: 3.6086\n",
      "Epoch: 9/10 Step: 20704... Loss: 2.6644... ppl: 36.6990  Val Loss: 3.6028\n",
      "Epoch: 9/10 Step: 20736... Loss: 2.4762... ppl: 36.9281  Val Loss: 3.6090\n",
      "Epoch: 9/10 Step: 20768... Loss: 2.9189... ppl: 36.9783  Val Loss: 3.6103\n",
      "Epoch: 9/10 Step: 20800... Loss: 2.8095... ppl: 36.9585  Val Loss: 3.6098\n",
      "Epoch: 9/10 Step: 20832... Loss: 2.7807... ppl: 36.9749  Val Loss: 3.6102\n",
      "Epoch: 9/10 Step: 20864... Loss: 2.3772... ppl: 37.0442  Val Loss: 3.6121\n",
      "Epoch: 9/10 Step: 20896... Loss: 2.7333... ppl: 36.8806  Val Loss: 3.6077\n",
      "Epoch: 9/10 Step: 20928... Loss: 2.4368... ppl: 36.7369  Val Loss: 3.6038\n",
      "Epoch: 9/10 Step: 20960... Loss: 2.6352... ppl: 36.7820  Val Loss: 3.6050\n",
      "Epoch: 9/10 Step: 20992... Loss: 2.2011... ppl: 37.6527  Val Loss: 3.6284\n",
      "Epoch: 9/10 Step: 21024... Loss: 2.3967... ppl: 37.4964  Val Loss: 3.6242\n",
      "Epoch: 9/10 Step: 21056... Loss: 2.6834... ppl: 37.1102  Val Loss: 3.6139\n",
      "Epoch: 10/10 Step: 21088... Loss: 2.7010... ppl: 36.7955  Val Loss: 3.6054\n",
      "Epoch: 10/10 Step: 21120... Loss: 2.4334... ppl: 37.1545  Val Loss: 3.6151\n",
      "Epoch: 10/10 Step: 21152... Loss: 2.6908... ppl: 37.1628  Val Loss: 3.6153\n",
      "Epoch: 10/10 Step: 21184... Loss: 2.7320... ppl: 37.1132  Val Loss: 3.6140\n",
      "Epoch: 10/10 Step: 21216... Loss: 2.2441... ppl: 37.2541  Val Loss: 3.6178\n",
      "Epoch: 10/10 Step: 21248... Loss: 2.4850... ppl: 37.1736  Val Loss: 3.6156\n",
      "Epoch: 10/10 Step: 21280... Loss: 3.1076... ppl: 37.4255  Val Loss: 3.6224\n",
      "Epoch: 10/10 Step: 21312... Loss: 2.7154... ppl: 37.4269  Val Loss: 3.6224\n",
      "Epoch: 10/10 Step: 21344... Loss: 2.7598... ppl: 37.5393  Val Loss: 3.6254\n",
      "Epoch: 10/10 Step: 21376... Loss: 2.4576... ppl: 37.5707  Val Loss: 3.6262\n",
      "Epoch: 10/10 Step: 21408... Loss: 2.5035... ppl: 37.8254  Val Loss: 3.6330\n",
      "Epoch: 10/10 Step: 21440... Loss: 2.7564... ppl: 37.5443  Val Loss: 3.6255\n",
      "Epoch: 10/10 Step: 21472... Loss: 2.5485... ppl: 37.2063  Val Loss: 3.6165\n",
      "Epoch: 10/10 Step: 21504... Loss: 2.5516... ppl: 37.1127  Val Loss: 3.6140\n",
      "Epoch: 10/10 Step: 21536... Loss: 2.3630... ppl: 37.3586  Val Loss: 3.6206\n",
      "Epoch: 10/10 Step: 21568... Loss: 2.7264... ppl: 37.7747  Val Loss: 3.6316\n",
      "Epoch: 10/10 Step: 21600... Loss: 2.3393... ppl: 37.7428  Val Loss: 3.6308\n",
      "Epoch: 10/10 Step: 21632... Loss: 2.3383... ppl: 37.6653  Val Loss: 3.6287\n",
      "Epoch: 10/10 Step: 21664... Loss: 2.5715... ppl: 37.6813  Val Loss: 3.6292\n",
      "Epoch: 10/10 Step: 21696... Loss: 2.9952... ppl: 37.7286  Val Loss: 3.6304\n",
      "Epoch: 10/10 Step: 21728... Loss: 2.3755... ppl: 37.7084  Val Loss: 3.6299\n",
      "Epoch: 10/10 Step: 21760... Loss: 2.1460... ppl: 37.7314  Val Loss: 3.6305\n",
      "Epoch: 10/10 Step: 21792... Loss: 2.2127... ppl: 37.9304  Val Loss: 3.6358\n",
      "Epoch: 10/10 Step: 21824... Loss: 2.4869... ppl: 37.8697  Val Loss: 3.6342\n",
      "Epoch: 10/10 Step: 21856... Loss: 2.5536... ppl: 37.8572  Val Loss: 3.6338\n",
      "Epoch: 10/10 Step: 21888... Loss: 2.3825... ppl: 37.8647  Val Loss: 3.6340\n",
      "Epoch: 10/10 Step: 21920... Loss: 2.9016... ppl: 38.0654  Val Loss: 3.6393\n",
      "Epoch: 10/10 Step: 21952... Loss: 2.7656... ppl: 38.2443  Val Loss: 3.6440\n",
      "Epoch: 10/10 Step: 21984... Loss: 1.9675... ppl: 38.0072  Val Loss: 3.6378\n",
      "Epoch: 10/10 Step: 22016... Loss: 2.7129... ppl: 37.8771  Val Loss: 3.6343\n",
      "Epoch: 10/10 Step: 22048... Loss: 2.2522... ppl: 38.0506  Val Loss: 3.6389\n",
      "Epoch: 10/10 Step: 22080... Loss: 2.7683... ppl: 38.0864  Val Loss: 3.6399\n",
      "Epoch: 10/10 Step: 22112... Loss: 2.1247... ppl: 37.8748  Val Loss: 3.6343\n",
      "Epoch: 10/10 Step: 22144... Loss: 2.2077... ppl: 38.1827  Val Loss: 3.6424\n",
      "Epoch: 10/10 Step: 22176... Loss: 2.3175... ppl: 37.9886  Val Loss: 3.6373\n",
      "Epoch: 10/10 Step: 22208... Loss: 2.8738... ppl: 38.0450  Val Loss: 3.6388\n",
      "Epoch: 10/10 Step: 22240... Loss: 2.5570... ppl: 37.9760  Val Loss: 3.6370\n",
      "Epoch: 10/10 Step: 22272... Loss: 2.7082... ppl: 37.6737  Val Loss: 3.6290\n",
      "Epoch: 10/10 Step: 22304... Loss: 2.6263... ppl: 37.7099  Val Loss: 3.6299\n",
      "Epoch: 10/10 Step: 22336... Loss: 2.4274... ppl: 37.8021  Val Loss: 3.6324\n",
      "Epoch: 10/10 Step: 22368... Loss: 3.0284... ppl: 37.7841  Val Loss: 3.6319\n",
      "Epoch: 10/10 Step: 22400... Loss: 2.5320... ppl: 37.8586  Val Loss: 3.6339\n",
      "Epoch: 10/10 Step: 22432... Loss: 2.3742... ppl: 37.9456  Val Loss: 3.6362\n",
      "Epoch: 10/10 Step: 22464... Loss: 1.7524... ppl: 38.3495  Val Loss: 3.6467\n",
      "Epoch: 10/10 Step: 22496... Loss: 2.5159... ppl: 38.1344  Val Loss: 3.6411\n",
      "Epoch: 10/10 Step: 22528... Loss: 2.9292... ppl: 38.1608  Val Loss: 3.6418\n",
      "Epoch: 10/10 Step: 22560... Loss: 2.5666... ppl: 38.0829  Val Loss: 3.6398\n",
      "Epoch: 10/10 Step: 22592... Loss: 2.3486... ppl: 37.8327  Val Loss: 3.6332\n",
      "Epoch: 10/10 Step: 22624... Loss: 2.8084... ppl: 37.8293  Val Loss: 3.6331\n",
      "Epoch: 10/10 Step: 22656... Loss: 2.3032... ppl: 38.2169  Val Loss: 3.6433\n",
      "Epoch: 10/10 Step: 22688... Loss: 2.8219... ppl: 38.1907  Val Loss: 3.6426\n",
      "Epoch: 10/10 Step: 22720... Loss: 2.3212... ppl: 38.1483  Val Loss: 3.6415\n",
      "Epoch: 10/10 Step: 22752... Loss: 2.8133... ppl: 37.9636  Val Loss: 3.6366\n",
      "Epoch: 10/10 Step: 22784... Loss: 2.7377... ppl: 38.1806  Val Loss: 3.6423\n",
      "Epoch: 10/10 Step: 22816... Loss: 2.5913... ppl: 38.1529  Val Loss: 3.6416\n",
      "Epoch: 10/10 Step: 22848... Loss: 2.6920... ppl: 38.0644  Val Loss: 3.6393\n",
      "Epoch: 10/10 Step: 22880... Loss: 2.0935... ppl: 38.4282  Val Loss: 3.6488\n",
      "Epoch: 10/10 Step: 22912... Loss: 1.9604... ppl: 38.4994  Val Loss: 3.6506\n",
      "Epoch: 10/10 Step: 22944... Loss: 2.8022... ppl: 38.4341  Val Loss: 3.6489\n",
      "Epoch: 10/10 Step: 22976... Loss: 2.4534... ppl: 38.2633  Val Loss: 3.6445\n",
      "Epoch: 10/10 Step: 23008... Loss: 2.7459... ppl: 38.2799  Val Loss: 3.6449\n",
      "Epoch: 10/10 Step: 23040... Loss: 2.3228... ppl: 37.9010  Val Loss: 3.6350\n",
      "Epoch: 10/10 Step: 23072... Loss: 2.6359... ppl: 37.9011  Val Loss: 3.6350\n",
      "Epoch: 10/10 Step: 23104... Loss: 2.5656... ppl: 38.0559  Val Loss: 3.6391\n",
      "Epoch: 10/10 Step: 23136... Loss: 2.4214... ppl: 38.0192  Val Loss: 3.6381\n",
      "Epoch: 10/10 Step: 23168... Loss: 2.7207... ppl: 38.1680  Val Loss: 3.6420\n",
      "Epoch: 10/10 Step: 23200... Loss: 2.3282... ppl: 38.1781  Val Loss: 3.6423\n",
      "Epoch: 10/10 Step: 23232... Loss: 2.4822... ppl: 38.1087  Val Loss: 3.6404\n",
      "Epoch: 10/10 Step: 23264... Loss: 2.5363... ppl: 38.0468  Val Loss: 3.6388\n",
      "Epoch: 10/10 Step: 23296... Loss: 2.5541... ppl: 37.8950  Val Loss: 3.6348\n",
      "Epoch: 10/10 Step: 23328... Loss: 2.4319... ppl: 38.6244  Val Loss: 3.6539\n",
      "Epoch: 10/10 Step: 23360... Loss: 2.9008... ppl: 38.7997  Val Loss: 3.6584\n",
      "Epoch: 10/10 Step: 23392... Loss: 2.7636... ppl: 38.4731  Val Loss: 3.6500\n",
      "Epoch: 10/10 Step: 23424... Loss: 2.0802... ppl: 38.1013  Val Loss: 3.6402\n"
     ]
    }
   ],
   "source": [
    "# define batch size\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "# train model\n",
    "train(net, batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load weights\n",
    "WEIGHTS_PATH = \"../module/models/saved_weights.pt\"\n",
    "net.load_state_dict(torch.load(WEIGHTS_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate one token\n",
    "def predict(net, tkn, h=None):\n",
    "    # tensor inputs\n",
    "    x = np.array([[token2int[tkn]]])\n",
    "    inputs = torch.from_numpy(x)\n",
    "\n",
    "    if (torch.cuda.is_available()):\n",
    "        inputs = inputs.cuda()\n",
    "    \n",
    "    # get output from model\n",
    "    out, h = net(inputs, h)\n",
    "    # get token probas\n",
    "    probas = F.softmax(out, dim=1).data\n",
    "\n",
    "    if (torch.cuda.is_available()):\n",
    "        probas = probas.cpu()\n",
    "    \n",
    "    probas = probas.numpy()\n",
    "    sampled_token_index = np.argmax(probas, axis=1)[0]\n",
    "\n",
    "    return int2token[sampled_token_index], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funtion to fetch generated sequence\n",
    "def sample(net, size=2, seed_text=''):\n",
    "    if (torch.cuda.is_available()):\n",
    "        net.cuda()\n",
    "    net.eval()\n",
    "\n",
    "    # batch size is 1\n",
    "    h = net.init_hidden(1)\n",
    "    toks = seed_text.split()\n",
    "\n",
    "    # predict next token\n",
    "    for t in toks:\n",
    "        token, h = predict(net, t, h)\n",
    "    toks.append(token)\n",
    "\n",
    "    # predict subsequent tokens\n",
    "    for i in range(size-1):\n",
    "        token, h = predict(net, toks[-1], h)\n",
    "        toks.append(token)\n",
    "    \n",
    "    return ' '.join(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed text:  i want to >> output:  i want to order a pizza from pizza hut\n",
      "\n",
      "\n",
      "seed text:  how about a cup >> output:  how about a cup of coffee for me from starbucks\n",
      "\n",
      "\n",
      "seed text:  i don't want >> output:  i don't want to drive it in today i\n",
      "\n",
      "\n",
      "seed text:  can you send >> output:  can you send me the confirmation to my email\n",
      "\n",
      "\n",
      "seed text:   my car >> output:  my car is making a weird knocking noise\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# seed texts\n",
    "seed_texts = [\n",
    "    \"i want to\",\n",
    "    \"how about a cup\",\n",
    "    \"i don't want\",\n",
    "    \"can you send\",\n",
    "    \" my car\"\n",
    "]\n",
    "# number of tokens to generate\n",
    "num_toks = 6\n",
    "\n",
    "# text generation\n",
    "for seed_t in seed_texts:\n",
    "    text_gen = sample(net, num_toks, seed_text=seed_t)\n",
    "    # print results\n",
    "    print(\"seed text: \", seed_t, \">> output: \", text_gen)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
